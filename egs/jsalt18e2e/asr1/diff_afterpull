8c8
< # as a target language, and use 10 Babel language (Cantonese Bengali Pashto Turkish Vietnamese
---
> # as a target language, and use 10 Babel language (Cantonese Bengali Pashto Turkish Tagalog Vietnamese
55a56,58
> # rnnlm related
> lm_weight=0.4
> 
67,85d69
< # data set
< # non-target languages: cantonese bengali pashto turkish vietnamese haitian tamil kurmanji tokpisin georgian
< train_set=tr_babel10
< train_dev=dt_babel10
< # non-target
< recog_set="dt_babel_cantonese et_babel_cantonese dt_babel_bengali et_babel_bengali dt_babel_pashto et_babel_pashto dt_babel_turkish et_babel_turkish\
<  dt_babel_vietnamese et_babel_vietnamese dt_babel_haitian et_babel_haitian\
<  dt_babel_tamil et_babel_tamil dt_babel_kurmanji et_babel_kurmanji dt_babel_tokpisin et_babel_tokpisin dt_babel_georgian et_babel_georgian"
< # target
< recog_set="dt_babel_assamese et_babel_assamese dt_babel_tagalog et_babel_tagalog dt_babel_swahili et_babel_swahili dt_babel_lao et_babel_lao dt_babel_zulu et_babel_zulu
<  dt_csj_japanese et_csj_japanese_1 et_csj_japanese_2 et_csj_japanese_3\
<  dt_libri_english_clean dt_libri_english_other et_libri_english_clean et_libri_english_other"
< # whole set
< recog_set="dt_babel_cantonese et_babel_cantonese dt_babel_assamese et_babel_assamese dt_babel_bengali et_babel_bengali dt_babel_pashto et_babel_pashto dt_babel_turkish et_babel_turkish\
<  dt_babel_vietnamese et_babel_vietnamese dt_babel_haitian et_babel_haitian dt_babel_swahili et_babel_swahili dt_babel_lao et_babel_lao dt_babel_tagalog et_babel_tagalog\
<  dt_babel_tamil et_babel_tamil dt_babel_kurmanji et_babel_kurmanji dt_babel_zulu et_babel_zulu dt_babel_tokpisin et_babel_tokpisin dt_babel_georgian et_babel_georgian\
<  dt_csj_japanese et_csj_japanese_1 et_csj_japanese_2 et_csj_japanese_3\
<  dt_libri_english_clean dt_libri_english_other et_libri_english_clean et_libri_english_other"
< 
112a97,114
> # non-target languages: cantonese bengali pashto turkish vietnamese haitian tamil kurmanji tokpisin georgian
> train_set=tr_babel10
> train_dev=dt_babel10
> # non-target
> recog_set="dt_babel_cantonese et_babel_cantonese dt_babel_bengali et_babel_bengali dt_babel_pashto et_babel_pashto dt_babel_turkish et_babel_turkish\
>  dt_babel_tagalog et_babel_tagalog dt_babel_vietnamese et_babel_vietnamese dt_babel_haitian et_babel_haitian\
>  dt_babel_tamil et_babel_tamil dt_babel_kurmanji et_babel_kurmanji dt_babel_tokpisin et_babel_tokpisin dt_babel_georgian et_babel_georgian"
> # target
> recog_set="dt_babel_assamese et_babel_assamese dt_babel_tagalog et_babel_tagalog dt_babel_swahili et_babel_swahili dt_babel_lao et_babel_lao dt_babel_zulu et_babel_zulu
>  dt_csj_japanese et_csj_japanese_1 et_csj_japanese_2 et_csj_japanese_3\
>  dt_libri_english_clean dt_libri_english_other et_libri_english_clean et_libri_english_other"
> # whole set
> recog_set="dt_babel_cantonese et_babel_cantonese dt_babel_assamese et_babel_assamese dt_babel_bengali et_babel_bengali dt_babel_pashto et_babel_pashto dt_babel_turkish et_babel_turkish\
>  dt_babel_vietnamese et_babel_vietnamese dt_babel_haitian et_babel_haitian dt_babel_swahili et_babel_swahili dt_babel_lao et_babel_lao\
>  dt_babel_tamil et_babel_tamil dt_babel_kurmanji et_babel_kurmanji dt_babel_zulu et_babel_zulu dt_babel_tokpisin et_babel_tokpisin dt_babel_georgian et_babel_georgian\
>  dt_csj_japanese et_csj_japanese_1 et_csj_japanese_2 et_csj_japanese_3\
>  dt_libri_english_clean dt_libri_english_other et_libri_english_clean et_libri_english_other"
> 
118c120
<     if [ ! -d "$csjdir/asr1/data" ]; then
---
>     if [ -d "$csjdir/asr1/data" ]; then
139c141
<     if [ ! -d "$libridir/asr1/data" ]; then
---
>     if [ -d "$libridir/asr1/data" ]; then
153c155
< 	if [ ! -d "$babeldir/asr1_${lang_code}/data" ]; then
---
> 	if [ -d "$babeldir/asr1_${lang_code}/data" ]; then
224a227,258
> # You can skip this and remove --rnnlm option in the recognition (stage 5)
> lmexpdir=exp/train_rnnlm_2layer_bs256
> mkdir -p ${lmexpdir}
> if [ ${stage} -le 3 ]; then
>     echo "stage 3: LM Preparation"
>     lmdatadir=data/local/lm_train
>     mkdir -p ${lmdatadir}
>     cat data/tr_{babel_*,csj*,libri*}/text | text2token.py -s 1 -n 1 -l ${nlsyms} | \
>         cut -f 2- -d" " | perl -pe 's/\n/ <eos> /g' > ${lmdatadir}/train.txt
>     cat data/dt_{babel_*,csj*,libri*}/text | text2token.py -s 1 -n 1 -l ${nlsyms} | \
>         cut -f 2- -d" " | perl -pe 's/\n/ <eos> /g' > ${lmdatadir}/valid.txt
>     # use only 1 gpu
>     if [ ${ngpu} -gt 1 ]; then
>         echo "LM training does not support multi-gpu. signle gpu will be used."
>         lmngpu=1
>     else
>         lmngpu=${ngpu}
>     fi
>     ${cuda_cmd} ${lmexpdir}/train.log \
>         lm_train.py \
>         --ngpu ${lmngpu} \
>         --backend ${backend} \
>         --verbose 1 \
>         --outdir ${lmexpdir} \
>         --train-label ${lmdatadir}/train.txt \
>         --valid-label ${lmdatadir}/valid.txt \
>         --epoch 40 \
>         --batchsize 256 \
>         --dict ${dict}
> fi
> 
> 
235,236c269,270
< if [ ${stage} -le 3 ]; then
<     echo "stage 3: Network Training"
---
> if [ ${stage} -le 4 ]; then
>     echo "stage 4: Network Training"
268,269c302,303
< if [ ${stage} -le 4 ]; then
<     echo "stage 4: Decoding"
---
> if [ ${stage} -le 5 ]; then
>     echo "stage 5: Decoding"
274c308
<         decode_dir=decode_${rtask}_beam${beam_size}_e${recog_model}_p${penalty}_len${minlenratio}-${maxlenratio}_ctcw${ctc_weight}
---
>         decode_dir=decode_${rtask}_beam${beam_size}_e${recog_model}_p${penalty}_len${minlenratio}-${maxlenratio}_ctcw${ctc_weight}_rnnlm${lm_weight}
303a338,339
>             --rnnlm ${lmexpdir}/rnnlm.model.best \
>             --lm-weight ${lm_weight} \
