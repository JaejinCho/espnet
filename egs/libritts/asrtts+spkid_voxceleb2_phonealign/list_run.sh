mkdir log

# 1. Data prep: spkloss_weight does not matter specified here in data prep.
## Generate symlinkcs for {dump,data,exp} directories to distribute data over nodes
mkdir -p /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/{dump,data,exp}
for dname in `ls -d /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/{dump,data,exp}`;do
    ln -s ${dname} $(basename ${dname})
done
## Actual run for data prep.
(DONE, *** only for the first run ***) bash run.asrttsspkid.sh --stage 0 --stop_stage 2 --ngpu 1 --n_average 0 --spkloss_weight 0.03 2>&1 | tee log/dataprep.stagefrom0to2.log

cp /export/b18/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb1_kaldiasr_phonealign_rf3/conf/train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56.yaml conf/
# 2. Training (2flstm512Nusecat + shufflebathcing (fixed bs) + fullyunsync with spkloss_weight=0.03 and nceloss=0.001)
## 1) SPKID only
(DONE) bash run.spkidonly.sh --train_config conf/train_pytorch_tacotron2+spkemb_spkidonly_shufflebatching_bs128.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --ttsloss_weight 0 --spkloss_weight 1 2>&1 | tee log/run.spkidonly.onlystage4.unsync.shufflebatching.bs128.log
(DONE, nceloss_weight=1) bash run.spkidonly_nceloss.sh --train_config conf/train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --ttsloss_weight 0 --spkloss_weight 1 --nceloss_weight 1 2>&1 | tee log/run.spkidonly_nceloss1.onlystage4.unsync.shufflebatching.bs76.log
(DONE, nceloss_weight=0.1) bash run.spkidonly_nceloss.sh --train_config conf/train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --ttsloss_weight 0 --spkloss_weight 1 --nceloss_weight 0.1 2>&1 | tee log/run.spkidonly_nceloss0.1.onlystage4.unsync.shufflebatching.bs76.log
(DONE, nceloss_weight=0.03) bash run.spkidonly_nceloss.sh --train_config conf/train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --ttsloss_weight 0 --spkloss_weight 1 --nceloss_weight 0.03 2>&1 | tee log/run.spkidonly_nceloss0.03.onlystage4.unsync.shufflebatching.bs76.log
(STOPPED DUE TO M ERROR, nceloss_weight=0.01) bash run.spkidonly_nceloss.sh --train_config conf/train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --ttsloss_weight 0 --spkloss_weight 1 --nceloss_weight 0.01 2>&1 | tee log/run.spkidonly_nceloss0.01.onlystage4.unsync.shufflebatching.bs76.log
(ING RESUMMED with self.it=395800 from ep.28, nceloss_weight=0.01) bash run.spkidonly_nceloss.sh --resume exp/voxceleb2_train_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76_spkloss_weight1_nceloss_weight0.01_unsync/results/snapshot.ep.28 --train_config conf/train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --ttsloss_weight 0 --spkloss_weight 1 --nceloss_weight 0.01 2>&1 | tee log/run.spkidonly_nceloss0.01.onlystage4.unsync.shufflebatching.bs76.resumedfromep.28.log
### 1) - 2 MoCo
#### W/O shuffleBN (***** Previously I ran the one below with bs76 but got some memory related error from torch so I increased cuda_memsize (this is mem size to be requested for cpu) from 5G to 20G)
(ING) bash run.spkidonly_nceloss.sh --train_config conf/train_pytorch_tacotron2+spkemb_spkidonly_nceloss_improve_reportnceval_shufflebatching_bs80.yaml --cuda_memsize 20 --nceloss_weight 1 --ngpu 1 --ttsloss_weight 0 --spkloss_weight 0 --stage 4 --n_average 0 --stop_stage 4 2>&1 | tee log/run.onlynceloss_improve_reportnceval.shufflebatching.bs80.chunks.from200to400.stage4only.log
#### W/ shuffleBN
(STOP by mistake) bash run.spkidonly_nceloss.sh --train_config conf/train_pytorch_tacotron2+spkemb_spkidonly_nceloss_improve_shuffleBN_reportnceval_shufflebatching_bs80.yaml --cuda_memsize 20 --nceloss_weight 1 --ngpu 1 --ttsloss_weight 0 --spkloss_weight 0 --stage 4 --n_average 0 --stop_stage 4 2>&1 | tee log/run.onlynceloss_improve_shuffleBN_reportnceval.shufflebatching.bs80.chunks.from200to400.stage4only.log
(ING: RESUME with self.it=174600) bash run.spkidonly_nceloss.sh --resume exp/voxceleb2_train_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_improve_shuffleBN_reportnceval_shufflebatching_bs80_spkloss_weight0_nceloss_weight1_unsync/results/snapshot.ep.13 --train_config conf/train_pytorch_tacotron2+spkemb_spkidonly_nceloss_improve_shuffleBN_reportnceval_shufflebatching_bs80.yaml --cuda_memsize 20 --nceloss_weight 1 --ngpu 1 --ttsloss_weight 0 --spkloss_weight 0 --stage 4 --n_average 0 --stop_stage 4 2>&1 | tee log/run.onlynceloss_improve_shuffleBN_reportnceval.shufflebatching.bs80.chunks.from200to400.stage4only.log
#### W/ shuffleBN + reduced overlap between two chunks from a same utterance
(ING) bash run.spkidonly_nceloss.sh --train_config conf/train_pytorch_tacotron2+spkemb_spkidonly_nceloss_improve_shuffleBN_reportnceval_shufflebatching_bs80_reduceoverlap.yaml --cuda_memsize 20 --nceloss_weight 1 --ttsloss_weight 0 --spkloss_weight 0 --stage 4 --n_average 0 --stop_stage 4 2>&1 | tee log/run.onlynceloss_improve_shuffleBN_reduceoverlap_reportnceval.shufflebatching.bs80.chunks.from200to400.stage4only.log

## 2) SPKID + TTS
### 2flstm512Nusecat
(DONE, spkloss_weight=0.03) bash run.asrttsspkid.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 2>&1 | tee log/run.asrttsspkid.spkloss_weight0.03.onlystage4.2flstm512Nusecat.fullyunsync.shufflebatching.bs64.log
(ING resume from the above with model.loss.best (snapshot.ep.16 model) with less learning rate (from 1e-3 to 1e-4) and self.it=268500) bash run.asrttsspkid.sh --resume exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0.03_fullyunsync/results/model.loss.best --train_config conf/train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_lr1e-4.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 2>&1 | tee log/run.asrttsspkid.spkloss_weight0.03.onlystage4.2flstm512Nusecat.fullyunsync.shufflebatching.bs64.resumeWlr1e-4.modellossbest.log
(DONE resume from the above with model.loss.best (snapshot.ep.16 model) with less learning rate (from 1e-3 to 1e-5) and self.it=268500) bash run.asrttsspkid.sh --resume exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0.03_fullyunsync/results/model.loss.best --train_config conf/train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_lr1e-5.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 2>&1 | tee log/run.asrttsspkid.spkloss_weight0.03.onlystage4.2flstm512Nusecat.fullyunsync.shufflebatching.bs64.resumeWlr1e-5.modellossbest.log
### Decrease spkloss_weight from after an epoch (ep.16)
(STOPPED (requested less memory) resume from the above with model.loss.best (snapshot.ep.16 model) with higher spkloss_weight (from 0.03 to 0.001, This is the same as you change lr only for spkid module from 1e-3 to 1e-4) and self.it=268500) bash run.asrttsspkid.sh --tag spklossw0.03to0.001 --resume exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0.03_fullyunsync/results/model.loss.best --train_config conf/train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.001 2>&1 | tee log/run.asrttsspkid.spkloss_weight0.001.onlystage4.2flstm512Nusecat.fullyunsync.shufflebatching.bs64.resumeWspkloss_weightfrom0.03to0.001.modellossbest.log
(ING resume from the above with model.loss.best (snapshot.ep.16 model) with higher spkloss_weight (from 0.03 to 0.003, This is the same as you change lr only for spkid module from 1e-3 to 1e-4) and self.it=268500) bash run.asrttsspkid.sh --tag spklossw0.03to0.003 --resume exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0.03_fullyunsync/results/model.loss.best --train_config conf/train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.003 2>&1 | tee log/run.asrttsspkid.spkloss_weight0.003.onlystage4.2flstm512Nusecat.fullyunsync.shufflebatching.bs64.resumeWspkloss_weightfrom0.03to0.003.modellossbest.log
(ING resume from the above with model.loss.best (snapshot.ep.16 model) with higher spkloss_weight (from 0.03 to 0.01, This is the same as you change lr only for spkid module from 1e-3 to 1e-4) and self.it=268500) bash run.asrttsspkid.sh --tag spklossw0.03to0.01 --resume exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0.03_fullyunsync/results/model.loss.best --train_config conf/train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.01 2>&1 | tee log/run.asrttsspkid.spkloss_weight0.01.onlystage4.2flstm512Nusecat.fullyunsync.shufflebatching.bs64.resumeWspkloss_weightfrom0.03to0.01.modellossbest.log
### Increase spkloss_weight from after an epoch (ep.16)
(STOP resume from the above with model.loss.best (snapshot.ep.16 model) with higher spkloss_weight (from 0.03 to 0.3, This is the same as you change lr only for spkid module from 1e-3 to 1e-4) and self.it=268500) bash run.asrttsspkid.sh --tag spklossw0.03to0.3 --resume exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0.03_fullyunsync/results/model.loss.best --train_config conf/train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.3 2>&1 | tee log/run.asrttsspkid.spkloss_weight0.3.onlystage4.2flstm512Nusecat.fullyunsync.shufflebatching.bs64.resumeWspkloss_weightfrom0.03to0.3.modellossbest.log
(STOP resume from the above with model.loss.best (snapshot.ep.16 model) with higher spkloss_weight (from 0.03 to 0.1, This is the same as you change lr only for spkid module from 1e-3 to 3.33e-3) and self.it=268500) bash run.asrttsspkid.sh --tag spklossw0.03to0.1 --resume exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0.03_fullyunsync/results/model.loss.best --train_config conf/train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.1 2>&1 | tee log/run.asrttsspkid.spkloss_weight0.1.onlystage4.2flstm512Nusecat.fullyunsync.shufflebatching.bs64.resumeWspkloss_weightfrom0.03to0.1.modellossbest.log
(STOP resume from the above with model.loss.best (snapshot.ep.16 model) with higher spkloss_weight (from 0.03 to 3, This is the same as you change lr only for spkid module from 1e-3 to 1e-5) and self.it=268500) bash run.asrttsspkid.sh --tag spklossw0.03to3 --resume exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0.03_fullyunsync/results/model.loss.best --train_config conf/train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 3 2>&1 | tee log/run.asrttsspkid.spkloss_weight3.onlystage4.2flstm512Nusecat.fullyunsync.shufflebatching.bs64.resumeWspkloss_weightfrom0.03to3.modellossbest.log
### With l2 regularization
(DONE, spkloss_weight=0.03 w/ l2 regularization=1e-6) bash run.asrttsspkid.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_l2reg1e-6.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 2>&1 | tee log/run.asrttsspkid.spkloss_weight0.03.onlystage4.2flstm512Nusecat.fullyunsync.shufflebatching.bs64.lereg1e-6.log
### With spkloss_weight=0 (TTS only)
(DONE, spkloss_weight=0) bash run.asrttsspkid.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0 2>&1 | tee log/run.asrttsspkid.spkloss_weight0.onlystage4.2flstm512Nusecat.fullyunsync.shufflebatching.bs64.log
#### with spkemb normalization before the concatenation (expecting better results for cosine distance scoring backend)
(ING, spkloss_weight=0) bash run.asrttsspkid.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkembnorm.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0 2>&1 | tee log/run.asrttsspkid.spkloss_weight0.onlystage4.2flstm512Nusecat.fullyunsync.shufflebatching.spkembnorm.bs64.log
### With spkloss_weight != 0
(STOP (NOT work), resume from the TTS only best model, self.it=268500 (mistake but it anyway makes self.lambda = 5 as desired), spkloss_weight=0.003, NO nceloss) bash run.asrttsspkid.sh --tag spkloss0to0.003 --resume exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0_fullyunsync/results/model.loss.best --train_config conf/train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.003 2>&1 | tee log/run.asrttsspkid.spkloss_weight0.003.onlystage4.2flstm512Nusecat.fullyunsync.shufflebatching.bs64.resumeWspkloss_weightfrom0to0.003.modellossbest.log
(STOP (NOT work), resume from the TTS only best model, self.it=268500, spkloss_weight=0.01, NO nceloss) bash run.asrttsspkid.sh --tag spkloss0to0.01 --resume exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0_fullyunsync/results/model.loss.best --train_config conf/train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.01 2>&1 | tee log/run.asrttsspkid.spkloss_weight0.01.onlystage4.2flstm512Nusecat.fullyunsync.shufflebatching.bs64.resumeWspkloss_weightfrom0to0.01.modellossbest.log
(ING, resume from the TTS only best model, self.it=268500, spkloss_weight=0.03, NO nceloss) bash run.asrttsspkid.sh --tag spkloss0to0.03 --resume exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0_fullyunsync/results/model.loss.best --train_config conf/train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 2>&1 | tee log/run.asrttsspkid.spkloss_weight0.03.onlystage4.2flstm512Nusecat.fullyunsync.shufflebatching.bs64.resumeWspkloss_weightfrom0to0.03.modellossbest.log
(STOP (NOT work), resume from the TTS only best model, self.it=268500, spkloss_weight=0.1, NO nceloss) bash run.asrttsspkid.sh --tag spkloss0to0.1 --resume exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0_fullyunsync/results/model.loss.best --train_config conf/train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.1 2>&1 | tee log/run.asrttsspkid.spkloss_weight0.1.onlystage4.2flstm512Nusecat.fullyunsync.shufflebatching.bs64.resumeWspkloss_weightfrom0to0.1.modellossbest.log
### 2flstm1024usecat
(STOP: M ERROR, spkloss_weight=0.01) bash run.asrttsspkid.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_fullyunsync_shufflebatching.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.01 2>&1 | tee log/run.asrttsspkid.spkloss_weight0.01.onlystage4.2flstm1024Nusecat.fullyunsync.shufflebatching.bs64.log
(ING: RESUMED from ep.16 with self.it=268600, spkloss_weight=0.01) bash run.asrttsspkid.sh --cuda_memsize 10 --resume exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_fullyunsync_shufflebatching_spkloss_weight0.01_fullyunsync/results/snapshot.ep.16 --train_config conf/train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_fullyunsync_shufflebatching.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.01 2>&1 | tee log/run.asrttsspkid.spkloss_weight0.01.onlystage4.2flstm1024Nusecat.fullyunsync.shufflebatching.bs64.resumedfromep.16.log
(DONE, spkloss_weight=0.03) bash run.asrttsspkid.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_fullyunsync_shufflebatching.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 2>&1 | tee log/run.asrttsspkid.spkloss_weight0.03.onlystage4.2flstm1024Nusecat.fullyunsync.shufflebatching.bs64.log
(DONE, spkloss_weight=0.1) bash run.asrttsspkid.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_fullyunsync_shufflebatching.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.1 2>&1 | tee log/run.asrttsspkid.spkloss_weight0.1.onlystage4.2flstm1024Nusecat.fullyunsync.shufflebatching.bs64.log
(DONE, spkloss_weight=0.3) bash run.asrttsspkid.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_fullyunsync_shufflebatching.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.3 2>&1 | tee log/run.asrttsspkid.spkloss_weight0.3.onlystage4.2flstm1024Nusecat.fullyunsync.shufflebatching.bs64.log
(ING, spkloss_weight=1) bash run.asrttsspkid.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_fullyunsync_shufflebatching.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 1 2>&1 | tee log/run.asrttsspkid.spkloss_weight1.onlystage4.2flstm1024Nusecat.fullyunsync.shufflebatching.bs64.log

(DONE, spkloss_weight=0.1, NO nceloss) bash run.asrttsspkid.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.1 2>&1 | tee log/run.asrttsspkid.spkloss_weight0.1.onlystage4.2flstm512Nusecat.fullyunsync.shufflebatching.bs64.log
(STOPPED after ep.19 (No improvement anyway), spkloss_weight=0.01, NO nceloss) bash run.asrttsspkid.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.01 2>&1 | tee log/run.asrttsspkid.spkloss_weight0.01.onlystage4.2flstm512Nusecat.fullyunsync.shufflebatching.bs64.log
## 3) TTS + SPKID + NCE
### nceloss_weight=0.01
(ING) bash run.asrttsspkid_nceloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --nceloss_weight 0.01 2>&1 | tee log/run.asrttsspkid_nceloss.spkloss_weight0.03.nceloss_weight0.01.onlystage4.2flstm512Nusecat.fullyunsync.shufflebatching.bs56.log
### nceloss_weight=0.003
(ING) bash run.asrttsspkid_nceloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --nceloss_weight 0.003 2>&1 | tee log/run.asrttsspkid_nceloss.spkloss_weight0.03.nceloss_weight0.003.onlystage4.2flstm512Nusecat.fullyunsync.shufflebatching.bs56.log
### nceloss_weight=0.001
(STOPPED, SORTING fix for chunks was wrong so was NOT converaging, still the best is ep.5) bash run.asrttsspkid_nceloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --nceloss_weight 0.001 2>&1 | tee log/run.asrttsspkid_nceloss.spkloss_weight0.03.nceloss_weight0.001.onlystage4.2flstm512Nusecat.fullyunsync.shufflebatching.bs56.log
(DONE: After the correct fix from the one above, the above run expdir was moved to exp/deprecated and this just uses the same expname) bash run.asrttsspkid_nceloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --nceloss_weight 0.001 2>&1 | tee log/run.asrttsspkid_nceloss.spkloss_weight0.03.nceloss_weight0.001.onlystage4.2flstm512Nusecat.fullyunsync.shufflebatching.bs56.log
(DONE resume from the above with model.loss.best (snapshot.ep.12 model) with less learning rate (from 1e-3 to 1e-4) and self.it=230200) bash run.asrttsspkid_nceloss.sh --resume exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56_spkloss_weight0.03_nceloss_weight0.001_fullyunsync/results/model.loss.best --train_config conf/train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56_lr1e-4.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --nceloss_weight 0.001 2>&1 | tee log/run.asrttsspkid_nceloss.spkloss_weight0.03.nceloss_weight0.001.onlystage4.2flstm512Nusecat.fullyunsync.shufflebatching.bs56.resumeWlr1e-4.modellossbest.log # It should be resuming from model.loss.best NOT snapshot.ep.* since currently if starting from a snapshot, it also gets the optimizer config. at the snapshot so different lr is not applied in resuming
(ING resume from the right above with model.loss.best (snapshot.ep.3 model) with less learning rate (from 1e-4 to 1e-5) and self.it=57700) bash run.asrttsspkid_nceloss.sh --resume exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56_lr1e-4_spkloss_weight0.03_nceloss_weight0.001_fullyunsync/results/model.loss.best --train_config conf/train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56_lr1e-5.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --nceloss_weight 0.001 2>&1 | tee log/run.asrttsspkid_nceloss.spkloss_weight0.03.nceloss_weight0.001.onlystage4.2flstm512Nusecat.fullyunsync.shufflebatching.bs56.resumeWlr1e-5.modellossbest.log
(DONE, spkloss_weight=0) bash run.asrttsspkid_nceloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0 --nceloss_weight 0.001 2>&1 | tee log/run.asrttsspkid_nceloss.spkloss_weight0.nceloss_weight0.001.onlystage4.2flstm512Nusecat.fullyunsync.shufflebatching.bs56.log
### 2flstm1024usecat
(STOPPED DUE TO OOM) bash run.asrttsspkid_nceloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_nceloss_fullyunsync_shufflebatching_bs56.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --nceloss_weight 0.001 2>&1 | tee log/run.asrttsspkid_nceloss.spkloss_weight0.03.nceloss_weight0.001.onlystage4.2flstm1024Nusecat.fullyunsync.shufflebatching.bs56.log
(DONE: "grad norm is nan" around ep.17) bash run.asrttsspkid_nceloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_nceloss_fullyunsync_shufflebatching_bs52.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --nceloss_weight 0.001 2>&1 | tee log/run.asrttsspkid_nceloss.spkloss_weight0.03.nceloss_weight0.001.onlystage4.2flstm1024Nusecat.fullyunsync.shufflebatching.bs52.log
(DONE RESUMMED with 1e-4 from ep.16 (the best alllossesBUTmseloss model), self.it=330600) bash run.asrttsspkid_nceloss.sh --resume exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_nceloss_fullyunsync_shufflebatching_bs52_spkloss_weight0.03_nceloss_weight0.001_fullyunsync/results/snapshot.ep.16 --train_config conf/train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_nceloss_fullyunsync_shufflebatching_bs52_lr1e-4.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --nceloss_weight 0.001 2>&1 | tee log/run.asrttsspkid_nceloss.spkloss_weight0.03.nceloss_weight0.001.onlystage4.2flstm1024Nusecat.fullyunsync.shufflebatching.bs52.resummedFromep16.from1e-3to1e-4.log
(RERUNNING RESUMMED with 1e-4 from ep.16 (the best alllossesBUTmseloss model), self.it=330600) bash run.asrttsspkid_nceloss.sh --resume exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_nceloss_fullyunsync_shufflebatching_bs52_spkloss_weight0.03_nceloss_weight0.001_fullyunsync/results/snapshot.ep.16 --train_config conf/train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_nceloss_fullyunsync_shufflebatching_bs52_lr1e-4.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --nceloss_weight 0.001 2>&1 | tee log/run.asrttsspkid_nceloss.spkloss_weight0.03.nceloss_weight0.001.onlystage4.2flstm1024Nusecat.fullyunsync.shufflebatching.bs52.resummedFromep16.from1e-3to1e-4.log
(RERUNNING RESUMMED with 1e-5 from ep.16 (the best alllossesBUTmseloss model), self.it=330600) bash run.asrttsspkid_nceloss.sh --resume exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_nceloss_fullyunsync_shufflebatching_bs52_spkloss_weight0.03_nceloss_weight0.001_fullyunsync/results/snapshot.ep.16 --train_config conf/train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_nceloss_fullyunsync_shufflebatching_bs52_lr1e-5.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --nceloss_weight 0.001 2>&1 | tee log/run.asrttsspkid_nceloss.spkloss_weight0.03.nceloss_weight0.001.onlystage4.2flstm1024Nusecat.fullyunsync.shufflebatching.bs52.resummedFromep16.from1e-3to1e-5.log
(RERUNNING RESUMMED with 1e-6 from ep.16 (the best alllossesBUTmseloss model), self.it=330600) bash run.asrttsspkid_nceloss.sh --resume exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_nceloss_fullyunsync_shufflebatching_bs52_spkloss_weight0.03_nceloss_weight0.001_fullyunsync/results/snapshot.ep.16 --train_config conf/train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_nceloss_fullyunsync_shufflebatching_bs52_lr1e-6.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --nceloss_weight 0.001 2>&1 | tee log/run.asrttsspkid_nceloss.spkloss_weight0.03.nceloss_weight0.001.onlystage4.2flstm1024Nusecat.fullyunsync.shufflebatching.bs52.resummedFromep16.from1e-3to1e-6.log
(RERUNNING RESUMMED with 1e-7 from ep.16 (the best alllossesBUTmseloss model), self.it=330600) bash run.asrttsspkid_nceloss.sh --resume exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_nceloss_fullyunsync_shufflebatching_bs52_spkloss_weight0.03_nceloss_weight0.001_fullyunsync/results/snapshot.ep.16 --train_config conf/train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_nceloss_fullyunsync_shufflebatching_bs52_lr1e-7.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --nceloss_weight 0.001 2>&1 | tee log/run.asrttsspkid_nceloss.spkloss_weight0.03.nceloss_weight0.001.onlystage4.2flstm1024Nusecat.fullyunsync.shufflebatching.bs52.resummedFromep16.from1e-3to1e-7.log


# 3. Backend

# 2) all voxceleb1 trials
## SPKID on voxceleb2
### (2.08)
(ING) bash run_backendevalonly_cpuparallel.voxceleb2plda_voxceleb1alltrialsetsEval.sh --nj 70 --expdir exp/voxceleb2_train_train_pytorch_tacotron2+spkemb_spkidonly_shufflebatching_bs128_spkloss_weight1_unsync/
## SPKID on voxceleb2Naug (voxceleb2Naug for embedding learning. Thus, cmvn.ark used for fbank normalization is NOT exactly what's used in embedding learning while assuming both are close)
### (1.92)
(DONE) bash run_backendevalonly_cpuparallel.voxceleb2plda_voxceleb1alltrialsetsEval.sh --model snapshot.ep.47 --nj 70 --expdir /export/b15/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2Naug_train_train_pytorch_tacotron2+spkemb_spkidonly_shufflebatching_bs128_spkloss_weight1_unsync/
## SPKID + NCE on voxceleb2
### (1.94)
(DONE) bash run_backendevalonly_cpuparallel.voxceleb2plda_voxceleb1alltrialsetsEval.sh --nj 70 --expdir exp/voxceleb2_train_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76_spkloss_weight1_nceloss_weight0.1_unsync/
## SPKID-TTS on voxceleb2Naug
### (2.00)
(DONE) bash run_backendevalonly_cpuparallel.voxceleb2plda_voxceleb1alltrialsetsEval.sh --model snapshot.ep.27 --nj 70 --expdir /export/b15/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2Naug_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_cleanrecon_lr1e-4_spkloss_weight0.03_fullyunsync_cleanrecon/
## SPKID-TTS + NCE on voxceleb2
### (1.90)
(DONE) bash run_backendevalonly_cpuparallel.voxceleb2plda_voxceleb1alltrialsetsEval.sh --model snapshot.ep.5 --nj 70 --expdir exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56_lr1e-4_spkloss_weight0.03_nceloss_weight0.001_fullyunsync/
### (1.94)
(DONE) bash run_backendevalonly_cpuparallel.voxceleb2plda_voxceleb1alltrialsetsEval.sh --model snapshot.ep.16 --nj 70 --expdir exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56_spkloss_weight0.03_nceloss_weight0.001_fullyunsync/
### (1.89, resumed with 1e-4 )
(DONE) bash run_backendevalonly_cpuparallel.voxceleb2plda_voxceleb1alltrialsetsEval.sh --model snapshot.ep.29 --nj 70 --expdir exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56_lr1e-4_spkloss_weight0.03_nceloss_weight0.001_fullyunsync/
## Combine embeddings from SPKID only + SPKID-TTS (1.94 + 1.89)
## Combine embeddings from SPKID only + TTS only (1.94 + )


# 1) voxceleb1_test
## TTS only
(DONE: 4.08, bestalllosses, snapshot.ep.29) bash run_backendonly_cpuparallel.voxceleb2plda_voxceleb1eval.sh --nj 70 --ngpu 0 --expdir exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0_fullyunsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0_fullyunsync.bestspkloss.snapshot.ep.29.log
## TTS only + nce
(DONE: 4.26, bestalllosses, snapshot.ep.17) bash run_backendonly_cpuparallel.voxceleb2plda_voxceleb1eval.sh --nj 70 --ngpu 0 --expdir exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56_spkloss_weight0_nceloss_weight0.001_fullyunsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56_spkloss_weight0_nceloss_weight0.001_fullyunsync.bestlossNreconlosses.snapshot.ep.17.log
(DONE: 6.05 , besttrainnceloss,  snapshot.ep.30 (expected since 30th val metrics are not marked)) bash run_backendonly_cpuparallel.voxceleb2plda_voxceleb1eval.sh --model snapshot.ep.30 --nj 70 --ngpu 0 --expdir exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56_spkloss_weight0_nceloss_weight0.001_fullyunsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56_spkloss_weight0_nceloss_weight0.001_fullyunsync.besttrainnceloss.snapshot.ep.30.log

## SPKID only
(DONE: 2.73, current bestspkloss, snapshot.ep.6) bash run_backendonly_cpuparallel.voxceleb2plda_voxceleb1eval.sh --model snapshot.ep.6 --nj 70 --ngpu 0 --expdir exp/voxceleb2_train_train_pytorch_tacotron2+spkemb_spkidonly_shufflebatching_bs128_spkloss_weight1_unsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_tacotron2+spkemb_spkidonly_shufflebatching_bs128_spkloss_weight1_unsync.bestspkloss.snapshot.ep.6.log
(DONE: 2.12, current bestspkloss, snapshot.ep.23) bash run_backendonly_cpuparallel.voxceleb2plda_voxceleb1eval.sh --model snapshot.ep.23 --nj 70 --ngpu 0 --expdir exp/voxceleb2_train_train_pytorch_tacotron2+spkemb_spkidonly_shufflebatching_bs128_spkloss_weight1_unsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_tacotron2+spkemb_spkidonly_shufflebatching_bs128_spkloss_weight1_unsync.bestspkloss.snapshot.ep.23.log
(DONE: 2.07, current bestspkloss among 30 epochs, snapshot.ep.29) bash run_backendonly_cpuparallel.voxceleb2plda_voxceleb1eval.sh --model snapshot.ep.29 --nj 70 --ngpu 0 --expdir exp/voxceleb2_train_train_pytorch_tacotron2+spkemb_spkidonly_shufflebatching_bs128_spkloss_weight1_unsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_tacotron2+spkemb_spkidonly_shufflebatching_bs128_spkloss_weight1_unsync.bestspkloss.snapshot.ep.29.log
(DONE: 2.14, current bestspkacc among 30 epochs, snapshot.ep.30) bash run_backendonly_cpuparallel.voxceleb2plda_voxceleb1eval.sh --model snapshot.ep.30 --nj 70 --ngpu 0 --expdir exp/voxceleb2_train_train_pytorch_tacotron2+spkemb_spkidonly_shufflebatching_bs128_spkloss_weight1_unsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_tacotron2+spkemb_spkidonly_shufflebatching_bs128_spkloss_weight1_unsync.bestspkacc.snapshot.ep.30.log
(DONE: 2.08, current bestspklossNspkacc among 50 epochs, snapshot.ep.49 or 50 (not sure since 50th metrics were not recorded)) bash run_backendonly_cpuparallel.voxceleb2plda_voxceleb1eval.sh --nj 70 --ngpu 0 --expdir exp/voxceleb2_train_train_pytorch_tacotron2+spkemb_spkidonly_shufflebatching_bs128_spkloss_weight1_unsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_tacotron2+spkemb_spkidonly_shufflebatching_bs128_spkloss_weight1_unsync.bestspkloss.log
### SPKID only + nce
#### nceloss_weight1
(DONE nceloss_weight1: 2.09, current bestspkloss, ep.38) bash run_backendonly_cpuparallel.voxceleb2plda_voxceleb1eval.sh --nj 70 --ngpu 0 --expdir exp/voxceleb2_train_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76_spkloss_weight1_nceloss_weight1_unsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76_spkloss_weight1_nceloss_weight1_unsync.currentbestspkloss.ep.38.log
(DONE: 2.05, bestspkidlossNpossiblespkidacc among 50 epochs, ep.47 (possibly ep.50)) bash run_backendonly_cpuparallel.voxceleb2plda_voxceleb1eval.sh --nj 70 --ngpu 0 --expdir exp/voxceleb2_train_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76_spkloss_weight1_nceloss_weight1_unsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76_spkloss_weight1_nceloss_weight1_unsync.bestspkidloss.log
#### nceloss_weight0.1
(DONE nceloss_weight0.1: 2.07, current bestspkloss, ep.26) bash run_backendonly_cpuparallel.voxceleb2plda_voxceleb1eval.sh --nj 70 --ngpu 0 --expdir exp/voxceleb2_train_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76_spkloss_weight1_nceloss_weight0.1_unsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76_spkloss_weight1_nceloss_weight0.1_unsync.currentbestspkloss.ep.26.log
(DONE: 1.94 nceloss_weight0.1, bestspkloss, ep.49 or 50) bash run_backendonly_cpuparallel.voxceleb2plda_voxceleb1eval.sh --nj 70 --ngpu 0 --expdir exp/voxceleb2_train_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76_spkloss_weight1_nceloss_weight0.1_unsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76_spkloss_weight1_nceloss_weight0.1_unsync.currentbestspkloss.ep.49.log
#### nceloss_weight0.03
(DONE nceloss_weight0.03: 2.09, current bestspkloss, ep.28) bash run_backendonly_cpuparallel.voxceleb2plda_voxceleb1eval.sh --nj 70 --ngpu 0 --expdir exp/voxceleb2_train_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76_spkloss_weight1_nceloss_weight0.03_unsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76_spkloss_weight1_nceloss_weight0.03_unsync.currentbestspkloss.ep.28.log
(DONE: 2.04 nceloss_weight0.03, bestspkloss, ep.49 or 50) bash run_backendonly_cpuparallel.voxceleb2plda_voxceleb1eval.sh --nj 70 --ngpu 0 --expdir exp/voxceleb2_train_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76_spkloss_weight1_nceloss_weight0.03_unsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76_spkloss_weight1_nceloss_weight0.03_unsync.bestspkloss.ep.49.log
### Voxceleb2Naug train (voxceleb2Naug for embedding learning. Thus, cmvn.ark used for fbank normalization is NOT exactly what's used in embedding learning while assuming both are close)
(DONE: 1.92, bestspkidacc, snapshot.ep.47) bash run_backendonly_cpuparallel.voxceleb2plda_voxceleb1eval.sh --model snapshot.ep.47 --nj 70 --ngpu 0 --expdir /export/b15/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2Naug_train_train_pytorch_tacotron2+spkemb_spkidonly_shufflebatching_bs128_spkloss_weight1_unsync/ 2>&1 | tee log/backend.voxceleb2Naug_train_train_pytorch_tacotron2+spkemb_spkidonly_shufflebatching_bs128_spkloss_weight1_unsync.bestspkidacc.snapshot.ep.47.log

## SPKID + TTS
### 0.01
(DONE: 2.11, bestlossNreconlosses, snapshot.ep.14 among 19 epochs) bash run_backendonly_cpuparallel.voxceleb2plda_voxceleb1eval.sh --model snapshot.ep.14 --nj 70 --ngpu 0 --expdir exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0.01_fullyunsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0.01_fullyunsync.bestlossNreconlosses.snapshot.ep.14.log
### 0.03
(DONE: 2.15, current bestlossNspkidlossNspkidacc, snapshot.ep.16) bash run_backendonly_cpuparallel.voxceleb2plda_voxceleb1eval.sh --model snapshot.ep.16 --nj 70 --ngpu 0 --expdir exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0.03_fullyunsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0.03_fullyunsync.bestloss.snapshot.ep.16.log
(DONE: 2.07, current bestl1Nmselosses (2nd bestloss), snapshot.ep.24) bash run_backendonly_cpuparallel.voxceleb2plda_voxceleb1eval.sh --model snapshot.ep.24 --nj 70 --ngpu 0 --expdir exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0.03_fullyunsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0.03_fullyunsync.bestl1andmselosses.2ndbestloss.snapshot.ep.24.log
(ING: 2.08, resume from the above with model.loss.best (snapshot.ep.16 model) with lr=1e-4, bestspkidlossNspkidacc, snapshot.ep.1) bash run_backendonly_cpuparallel.voxceleb2plda_voxceleb1eval.sh --model snapshot.ep.1 --nj 70 --ngpu 0 --expdir exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_lr1e-4_spkloss_weight0.03_fullyunsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_lr1e-4_spkloss_weight0.03_fullyunsync.bestspkidlossNspkidacc.snapshot.ep.1.log
(ING: 2.08, resume from the above with model.loss.best (snapshot.ep.16 model) with lr=1e-4, bestloss, snapshot.ep.4) bash run_backendonly_cpuparallel.voxceleb2plda_voxceleb1eval.sh --model snapshot.ep.4 --nj 70 --ngpu 0 --expdir exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_lr1e-4_spkloss_weight0.03_fullyunsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_lr1e-4_spkloss_weight0.03_fullyunsync.bestloss.snapshot.ep.4.log
(ING: 2.06, resume from the above with model.loss.best (snapshot.ep.16 model) with lr=1e-4, bestreconloss, snapshot.ep.17) bash run_backendonly_cpuparallel.voxceleb2plda_voxceleb1eval.sh --model snapshot.ep.17 --nj 70 --ngpu 0 --expdir exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_lr1e-4_spkloss_weight0.03_fullyunsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_lr1e-4_spkloss_weight0.03_fullyunsync.bestreconloss.snapshot.ep.17.log
#### lr=1e-5
(DONE: 2.07, lr=1e-5 from lr=1e-3, bestreconlosses) bash run_backendonly_cpuparallel.voxceleb2plda_voxceleb1eval.sh --model snapshot.ep.27 --nj 70 --ngpu 0 --expdir exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_lr1e-5_spkloss_weight0.03_fullyunsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_lr1e-5_spkloss_weight0.03_fullyunsync.bestreconlosses.ep.27.log
#### + l2reg1e-6
(DONE: 2.05, bestlossNmseloss, ep.18) bash run_backendonly_cpuparallel.voxceleb2plda_voxceleb1eval.sh --nj 70 --ngpu 0 --expdir exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_l2reg1e-6_spkloss_weight0.03_fullyunsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_l2reg1e-6_spkloss_weight0.03_fullyunsync.bestlossNmseloss.ep.18.log
(DONE: 2.13, bestspkidlossNspkidacc, ep.15) bash run_backendonly_cpuparallel.voxceleb2plda_voxceleb1eval.sh --model snapshot.ep.15 --nj 70 --ngpu 0 --expdir exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_l2reg1e-6_spkloss_weight0.03_fullyunsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_l2reg1e-6_spkloss_weight0.03_fullyunsync.bestspkidlossNspkidacc.ep.15.log
#### SPKID + TTS trained with voxceleb2Naug (voxceleb2Naug for embedding learning. Thus, cmvn.ark used for fbank normalization is NOT exactly what's used in embedding learning while assuming both are close)
(DONE: 2.00, bestmseloss, snapshot.ep.27) bash run_backendonly_cpuparallel.voxceleb2plda_voxceleb1eval.sh --model snapshot.ep.27 --nj 70 --ngpu 0 --expdir /export/b15/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2Naug_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_cleanrecon_lr1e-4_spkloss_weight0.03_fullyunsync_cleanrecon/ 2>&1 | tee log/backend.voxceleb2Naug_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_cleanrecon_lr1e-4_spkloss_weight0.03_fullyunsync_cleanrecon.bestmseloss.snapshot.ep.27.log
### 0.1
(DONE: 2.53, bestlossNspkacc, snapshot.ep.5 among 30 epochs) bash run_backendonly_cpuparallel.voxceleb2plda_voxceleb1eval.sh --model snapshot.ep.5 --nj 70 --ngpu 0 --expdir exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0.1_fullyunsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0.1_fullyunsync.bestlossNspkacc.snapshot.ep.5.log

## SPKID + TTS + NCE: Correct fix
### nceloss_weight=0.001
(DONE: 2.08, bestspkidloss, snapshot.ep.12) bash run_backendonly_cpuparallel.voxceleb2plda_voxceleb1eval.sh --model snapshot.ep.12 --nj 70 --ngpu 0 --expdir exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56_spkloss_weight0.03_nceloss_weight0.001_fullyunsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56_spkloss_weight0.03_nceloss_weight0.001_fullyunsync.bestloss.snapshot.ep.12.log
(DONE: 1.94, bestlossNreconlosses, snapshot.ep.16) bash run_backendonly_cpuparallel.voxceleb2plda_voxceleb1eval.sh --model snapshot.ep.16 --nj 70 --ngpu 0 --expdir exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56_spkloss_weight0.03_nceloss_weight0.001_fullyunsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56_spkloss_weight0.03_nceloss_weight0.001_fullyunsync.bestlossNreconlosses.snapshot.ep.16.log
(DONE: 2.09, bestspkidacc, snapshot.ep.24) bash run_backendonly_cpuparallel.voxceleb2plda_voxceleb1eval.sh --model snapshot.ep.24 --nj 70 --ngpu 0 --expdir exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56_spkloss_weight0.03_nceloss_weight0.001_fullyunsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56_spkloss_weight0.03_nceloss_weight0.001_fullyunsync.bestspkidacc.snapshot.ep.24.log
#### lr=1e-4
(DONE: 1.92, resume from ep12 with lr=1e-4, bestlossNspkidloss, snapshot.ep.3) bash run_backendonly_cpuparallel.voxceleb2plda_voxceleb1eval.sh --model snapshot.ep.3 --nj 70 --ngpu 0 --expdir exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56_lr1e-4_spkloss_weight0.03_nceloss_weight0.001_fullyunsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56_lr1e-4_spkloss_weight0.03_nceloss_weight0.001_fullyunsync.bestloss.snapshot.ep.3.log
(DONE: 1.90, resume from ep12 with lr=1e-4, bestspkidacc, snapshot.ep.5) bash run_backendonly_cpuparallel.voxceleb2plda_voxceleb1eval.sh --model snapshot.ep.5 --nj 70 --ngpu 0 --expdir exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56_lr1e-4_spkloss_weight0.03_nceloss_weight0.001_fullyunsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56_lr1e-4_spkloss_weight0.03_nceloss_weight0.001_fullyunsync.bestspkidacc.snapshot.ep.5.log
(DONE: 1.89, resume from ep12 with lr=1e-4, bestreconlosses, snapshot.ep.29) bash run_backendonly_cpuparallel.voxceleb2plda_voxceleb1eval.sh --model snapshot.ep.29 --nj 70 --ngpu 0 --expdir exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56_lr1e-4_spkloss_weight0.03_nceloss_weight0.001_fullyunsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56_lr1e-4_spkloss_weight0.03_nceloss_weight0.001_fullyunsync.bestreconlosses.snapshot.ep.29.log
#### lr=1e-5
(DONE: 1.91, lr=1e-5 from lr=1e-4) bash run_backendonly_cpuparallel.voxceleb2plda_voxceleb1eval.sh --model snapshot.ep.29 --nj 70 --ngpu 0 --expdir exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56_lr1e-5_spkloss_weight0.03_nceloss_weight0.001_fullyunsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56_lr1e-5_spkloss_weight0.03_nceloss_weight0.001_fullyunsync.bestl1loss.ep.29.log
### nceloss_weight=0.003
(DONE: 2.03, current lossbest ep.14) bash run_backendonly_cpuparallel.voxceleb2plda_voxceleb1eval.sh --model snapshot.ep.14 --nj 70 --ngpu 0 --expdir exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56_spkloss_weight0.03_nceloss_weight0.003_fullyunsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56_spkloss_weight0.03_nceloss_weight0.003_fullyunsync.currentlossbest.snapshot.ep.14.log
### nceloss_weight=0.01
(DONE: 2.30, current lossbest ep.19) bash run_backendonly_cpuparallel.voxceleb2plda_voxceleb1eval.sh --model snapshot.ep.19 --nj 70 --ngpu 0 --expdir exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56_spkloss_weight0.03_nceloss_weight0.01_fullyunsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56_spkloss_weight0.03_nceloss_weight0.01_fullyunsync.currentlossbest.snapshot.ep.19.log

## (DISREGARD) SPKID + TTS + NCE: Wrong
(DONE: 2.37, current bestloss, snapshot.ep.5) bash run_backendonly_cpuparallel.voxceleb2plda_voxceleb1eval.sh --nj 70 --ngpu 0 --expdir exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56_spkloss_weight0.03_nceloss_weight0.001_fullyunsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56_spkloss_weight0.03_nceloss_weight0.001_fullyunsync.log
(DONE: 2.16, current bestloss after jerk upwards), snapshot.ep.12) bash run_backendonly_cpuparallel.voxceleb2plda_voxceleb1eval.sh --model snapshot.ep.12 --nj 70 --ngpu 0 --expdir exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56_spkloss_weight0.03_nceloss_weight0.001_fullyunsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56_spkloss_weight0.03_nceloss_weight0.001_fullyunsync.bestlossafterjerkupwards.snapshot.ep.12.log

# 4. Check results
expdir=''
paste <(valloss ${expdir}/results/log | sort -nk3) <(valspkloss ${expdir}/results/log | sort -nk3) <(valspkacc ${expdir}/results/log | sort -rnk3)
paste <(valloss ${expdir}/results/log | sort -nk3) <(vall1loss ${expdir}/results/log | sort -nk3) <(valmseloss ${expdir}/results/log | sort -nk3)

showspk () { local expdir=$1; paste <(valloss ${expdir}/results/log | sort -nk3) <(valspkloss ${expdir}/results/log | sort -nk3) <(valspkacc ${expdir}/results/log | sort -rnk3); }
showrecon () { local expdir=$1; paste <(valloss ${expdir}/results/log | sort -nk3) <(vall1loss ${expdir}/results/log | sort -nk3) <(valmseloss ${expdir}/results/log | sort -nk3); }
showspkn () { local expdir=$1; local n=$2; paste <(valloss ${expdir}/results/log | head -n${n} | sort -nk3) <(valspkloss ${expdir}/results/log | head -n${n} | sort -nk3) <(valspkacc ${expdir}/results/log | head -n${n} | sort -rnk3); }
showreconn () { local expdir=$1; local n=$2; paste <(valloss ${expdir}/results/log | head -n${n} | sort -nk3) <(vall1loss ${expdir}/results/log | head -n${n} | sort -nk3) <(valmseloss ${expdir}/results/log | head -n${n} | sort -nk3); }
