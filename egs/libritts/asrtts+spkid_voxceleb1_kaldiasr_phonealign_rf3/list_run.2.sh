# Outline: Run commands for (spkid)-tts with FT2 best config. & spkidonly exp.

# 1. Front-end
## Finished job example TTS + SPKID
(DONE, fixed bs w/ shuf. fullyunsync 2flstm512 & use_concate=true) bash run.asrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 2>&1 | tee log/run.asrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.03.onlystage4.forwardtacotron2_2flstm512Nusecat_fullyunsync_shufflebatching.log
### consloss
< l1 loss >
#### vanila
(DONE, bs60) bash run.asrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.consloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --consloss_weight 10 2>&1 | tee log/run.asrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.03.onlystage4.forwardtacotron2_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_weight10.log
(DONE, bs60) bash run.asrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.consloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --consloss_weight 1 2>&1 | tee log/run.asrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.03.onlystage4.forwardtacotron2_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_weight1.log
(DONE, bs60) bash run.asrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.consloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --consloss_weight 0.1 2>&1 | tee log/run.asrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.03.onlystage4.forwardtacotron2_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_weight0.1.log
(DONE, bs60) bash run.asrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.consloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --consloss_weight 0.01 2>&1 | tee log/run.asrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.03.onlystage4.forwardtacotron2_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_weight0.01.log
#### vanila + (normalized spkemb for dec & consloss. *** Reasoning behind this fix: I thought if we do not normalize the speaker embeddings for consloss, just scaling down speaker embeddings will lead to reduce l1 consloss)
(DONE, bs60) bash run.asrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.consloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_normspkemb.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --consloss_weight 10 2>&1 | tee log/run.asrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.03.onlystage4.forwardtacotron2_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_normspkemb_weight10.log
(DONE, bs60) bash run.asrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.consloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_normspkemb.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --consloss_weight 1 2>&1 | tee log/run.asrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.03.onlystage4.forwardtacotron2_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_normspkemb_weight1.log
(DONE, bs60) bash run.asrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.consloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_normspkemb.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --consloss_weight 0.1 2>&1 | tee log/run.asrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.03.onlystage4.forwardtacotron2_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_normspkemb_weight0.1.log
(DONE, bs60) bash run.asrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.consloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_normspkemb.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --consloss_weight 0.01 2>&1 | tee log/run.asrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.03.onlystage4.forwardtacotron2_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_normspkemb_weight0.01.log
#### detach ref
(DONE, bs60) bash run.asrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.consloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_detach_ref.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --consloss_weight 10 2>&1 | tee log/run.asrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.03.onlystage4.forwardtacotron2_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_detach_ref_weight10.log
(DONE, bs60) bash run.asrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.consloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_detach_ref.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --consloss_weight 1 2>&1 | tee log/run.asrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.03.onlystage4.forwardtacotron2_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_detach_ref_weight1.log
(DONE, bs60) bash run.asrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.consloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_detach_ref.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --consloss_weight 0.1 2>&1 | tee log/run.asrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.03.onlystage4.forwardtacotron2_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_detach_ref_weight0.1.log
(DONE, bs60) bash run.asrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.consloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_detach_ref.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --consloss_weight 0.01 2>&1 | tee log/run.asrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.03.onlystage4.forwardtacotron2_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_detach_ref_weight0.01.log
#### detach ref + (normalized spkemb in consloss)
(DONE, bs60) bash run.asrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.consloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_detach_ref_normspkemb.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --consloss_weight 10 2>&1 | tee log/run.asrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.03.onlystage4.forwardtacotron2_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_detach_ref_normspkemb_weight10.log
(DONE, bs60) bash run.asrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.consloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_detach_ref_normspkemb.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --consloss_weight 1 2>&1 | tee log/run.asrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.03.onlystage4.forwardtacotron2_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_detach_ref_normspkemb_weight1.log
(DONE, bs60) bash run.asrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.consloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_detach_ref_normspkemb.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --consloss_weight 0.1 2>&1 | tee log/run.asrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.03.onlystage4.forwardtacotron2_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_detach_ref_normspkemb_weight0.1.log
(DONE, bs60) bash run.asrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.consloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_detach_ref_normspkemb.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --consloss_weight 0.01 2>&1 | tee log/run.asrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.03.onlystage4.forwardtacotron2_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_detach_ref_normspkemb_weight0.01.log
#### detach syn
(DONE, bs60) bash run.asrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.consloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_detach_syn.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --consloss_weight 10 2>&1 | tee log/run.asrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.03.onlystage4.forwardtacotron2_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_detach_syn_weight10.log
(DONE, bs60) bash run.asrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.consloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_detach_syn.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --consloss_weight 1 2>&1 | tee log/run.asrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.03.onlystage4.forwardtacotron2_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_detach_syn_weight1.log
(DONE, bs60) bash run.asrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.consloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_detach_syn.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --consloss_weight 0.1 2>&1 | tee log/run.asrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.03.onlystage4.forwardtacotron2_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_detach_syn_weight0.1.log
(DONE, bs60) bash run.asrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.consloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_detach_syn.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --consloss_weight 0.01 2>&1 | tee log/run.asrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.03.onlystage4.forwardtacotron2_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_detach_syn_weight0.01.log
#### detach syn + (normalized spkemb in consloss)
(DONE, bs60) bash run.asrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.consloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_detach_syn_normspkemb.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --consloss_weight 10 2>&1 | tee log/run.asrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.03.onlystage4.forwardtacotron2_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_detach_syn_normspkemb_weight10.log
(DONE, bs60) bash run.asrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.consloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_detach_syn_normspkemb.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --consloss_weight 1 2>&1 | tee log/run.asrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.03.onlystage4.forwardtacotron2_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_detach_syn_normspkemb_weight1.log
(DONE, bs60) bash run.asrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.consloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_detach_syn_normspkemb.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --consloss_weight 0.1 2>&1 | tee log/run.asrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.03.onlystage4.forwardtacotron2_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_detach_syn_normspkemb_weight0.1.log
(DONE, bs60) bash run.asrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.consloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_detach_syn_normspkemb.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --consloss_weight 0.01 2>&1 | tee log/run.asrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.03.onlystage4.forwardtacotron2_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_detach_syn_normspkemb_weight0.01.log
< cosine similarity >
#### vanila
(ING, bs60) bash run.asrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.consloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_cosdis.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --consloss_weight 10 2>&1 | tee log/run.asrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.03.onlystage4.forwardtacotron2_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_cosdis_weight10.log
(ING, bs60) bash run.asrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.consloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_cosdis.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --consloss_weight 1 2>&1 | tee log/run.asrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.03.onlystage4.forwardtacotron2_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_cosdis_weight1.log
(ING, bs60) bash run.asrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.consloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_cosdis.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --consloss_weight 0.1 2>&1 | tee log/run.asrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.03.onlystage4.forwardtacotron2_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_cosdis_weight0.1.log
(ING, bs60) bash run.asrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.consloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_cosdis.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --consloss_weight 0.01 2>&1 | tee log/run.asrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.03.onlystage4.forwardtacotron2_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_cosdis_weight0.01.log
#### detach ref
(ING, bs60) bash run.asrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.consloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_cosdis_detach_ref.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --consloss_weight 10 2>&1 | tee log/run.asrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.03.onlystage4.forwardtacotron2_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_cosdis_detach_ref_weight10.log
(ING, bs60) bash run.asrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.consloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_cosdis_detach_ref.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --consloss_weight 1 2>&1 | tee log/run.asrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.03.onlystage4.forwardtacotron2_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_cosdis_detach_ref_weight1.log
(ING, bs60) bash run.asrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.consloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_cosdis_detach_ref.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --consloss_weight 0.1 2>&1 | tee log/run.asrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.03.onlystage4.forwardtacotron2_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_cosdis_detach_ref_weight0.1.log
(ING, bs60) bash run.asrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.consloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_cosdis_detach_ref.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --consloss_weight 0.01 2>&1 | tee log/run.asrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.03.onlystage4.forwardtacotron2_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_cosdis_detach_ref_weight0.01.log
#### detach syn
(ING, bs60) bash run.asrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.consloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_cosdis_detach_syn.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --consloss_weight 10 2>&1 | tee log/run.asrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.03.onlystage4.forwardtacotron2_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_cosdis_detach_syn_weight10.log
(ING, bs60) bash run.asrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.consloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_cosdis_detach_syn.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --consloss_weight 1 2>&1 | tee log/run.asrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.03.onlystage4.forwardtacotron2_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_cosdis_detach_syn_weight1.log
(ING, bs60) bash run.asrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.consloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_cosdis_detach_syn.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --consloss_weight 0.1 2>&1 | tee log/run.asrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.03.onlystage4.forwardtacotron2_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_cosdis_detach_syn_weight0.1.log
(ING, bs60) bash run.asrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.consloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_cosdis_detach_syn.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --consloss_weight 0.01 2>&1 | tee log/run.asrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.03.onlystage4.forwardtacotron2_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_cosdis_detach_syn_weight0.01.log



## spkid only
(DONE) bash run.spkidonly.sh --train_config conf/train_pytorch_tacotron2+spkemb_spkidonly_shufflebatching_bs128_lr1e-5_ep110.yaml --resume exp/voxceleb1_train_filtered_train_pytorch_train_pytorch_tacotron2+spkemb_spkidonly_shufflebatching_bs128_lr1e-4_ep80_speakeridonly_ttslossw0_spkloss_weight1_final_unsync200to400random/results/snapshot.ep.74 --ngpu 1 --ttsloss_weight 0 --spkloss_weight 1 --stage 4 --n_average 0 --stop_stage 4 2>&1 | tee log/run.spkidonly.shufflebatching.bs128.chunks.from200to400.stage4only.extendto110epfrom74ep.log
#### + 3-dim pitch feature
(ING) bash run_3pitchadd.spkidonly.sh --ngpu 1 --ttsloss_weight 0 --spkloss_weight 1 --n_average 0 --stop_stage 4 2>&1 | tee log/run_3pitchadd.spkidonly.shufflebatching.bs128.chunks.from200to400.log

## SPKID only + NCE (chunk training)
(DONE) bash run.spkidonly_nceloss.sh --nceloss_weight 0.003 --ngpu 1 --ttsloss_weight 0 --spkloss_weight 1 --stage 4 --n_average 0 --stop_stage 4 2>&1 | tee log/run.spkidonly_nceloss0.003.shufflebatching.bs76.chunks.from200to400.stage4only.log
(DONE) bash run.spkidonly_nceloss.sh --nceloss_weight 0.01 --ngpu 1 --ttsloss_weight 0 --spkloss_weight 1 --stage 4 --n_average 0 --stop_stage 4 2>&1 | tee log/run.spkidonly_nceloss0.01.shufflebatching.bs76.chunks.from200to400.stage4only.log
(DONE) bash run.spkidonly_nceloss.sh --nceloss_weight 0.03 --ngpu 1 --ttsloss_weight 0 --spkloss_weight 1 --stage 4 --n_average 0 --stop_stage 4 2>&1 | tee log/run.spkidonly_nceloss0.03.shufflebatching.bs76.chunks.from200to400.stage4only.log
(DONE) bash run.spkidonly_nceloss.sh --nceloss_weight 0.1 --ngpu 1 --ttsloss_weight 0 --spkloss_weight 1 --stage 4 --n_average 0 --stop_stage 4 2>&1 | tee log/run.spkidonly_nceloss0.1.shufflebatching.bs76.chunks.from200to400.stage4only.log
(DONE) bash run.spkidonly_nceloss.sh --nceloss_weight 0.3 --ngpu 1 --ttsloss_weight 0 --spkloss_weight 1 --stage 4 --n_average 0 --stop_stage 4 2>&1 | tee log/run.spkidonly_nceloss0.3.shufflebatching.bs76.chunks.from200to400.stage4only.log
### improved NCE (with MoCo): Only change happends to --train_config from the above
(DONE) bash run.spkidonly_nceloss.sh --train_config conf/train_pytorch_tacotron2+spkemb_spkidonly_nceloss_improve_shufflebatching_bs76.yaml --nceloss_weight 1 --ngpu 1 --ttsloss_weight 0 --spkloss_weight 1 --stage 4 --n_average 0 --stop_stage 4 2>&1 | tee log/run.spkidonly_nceloss_improve1.shufflebatching.bs76.chunks.from200to400.stage4only.log
(DONE) bash run.spkidonly_nceloss.sh --train_config conf/train_pytorch_tacotron2+spkemb_spkidonly_nceloss_improve_shufflebatching_bs76.yaml --nceloss_weight 0.1 --ngpu 1 --ttsloss_weight 0 --spkloss_weight 1 --stage 4 --n_average 0 --stop_stage 4 2>&1 | tee log/run.spkidonly_nceloss_improve0.1.shufflebatching.bs76.chunks.from200to400.stage4only.log
(DONE) bash run.spkidonly_nceloss.sh --train_config conf/train_pytorch_tacotron2+spkemb_spkidonly_nceloss_improve_shufflebatching_bs76.yaml --nceloss_weight 0.01 --ngpu 1 --ttsloss_weight 0 --spkloss_weight 1 --stage 4 --n_average 0 --stop_stage 4 2>&1 | tee log/run.spkidonly_nceloss_improve0.01.shufflebatching.bs76.chunks.from200to400.stage4only.log
(DONE) bash run.spkidonly_nceloss.sh --train_config conf/train_pytorch_tacotron2+spkemb_spkidonly_nceloss_improve_shufflebatching_bs76.yaml --nceloss_weight 1 --ngpu 1 --ttsloss_weight 0 --spkloss_weight 0 --stage 4 --n_average 0 --stop_stage 4 2>&1 | tee log/run.onlynceloss_improve.shufflebatching.bs76.chunks.from200to400.stage4only.log
#### (+) nce_{acc,loss} added to validation phase too
(DONE) bash run.spkidonly_nceloss.sh --train_config conf/train_pytorch_tacotron2+spkemb_spkidonly_nceloss_improve_reportnceval_shufflebatching_bs76.yaml --nceloss_weight 1 --ngpu 1 --ttsloss_weight 0 --spkloss_weight 1 --stage 4 --n_average 0 --stop_stage 4 2>&1 | tee log/run.spkidonly_nceloss_improve_reportnceval1.shufflebatching.bs76.chunks.from200to400.stage4only.log
(DONE) bash run.spkidonly_nceloss.sh --train_config conf/train_pytorch_tacotron2+spkemb_spkidonly_nceloss_improve_reportnceval_shufflebatching_bs76.yaml --nceloss_weight 0.1 --ngpu 1 --ttsloss_weight 0 --spkloss_weight 1 --stage 4 --n_average 0 --stop_stage 4 2>&1 | tee log/run.spkidonly_nceloss_improve_reportnceval0.1.shufflebatching.bs76.chunks.from200to400.stage4only.log
(DONE) bash run.spkidonly_nceloss.sh --train_config conf/train_pytorch_tacotron2+spkemb_spkidonly_nceloss_improve_reportnceval_shufflebatching_bs76.yaml --nceloss_weight 0.01 --ngpu 1 --ttsloss_weight 0 --spkloss_weight 1 --stage 4 --n_average 0 --stop_stage 4 2>&1 | tee log/run.spkidonly_nceloss_improve_reportnceval0.01.shufflebatching.bs76.chunks.from200to400.stage4only.log
(DONE) bash run.spkidonly_nceloss.sh --train_config conf/train_pytorch_tacotron2+spkemb_spkidonly_nceloss_improve_reportnceval_shufflebatching_bs76.yaml --nceloss_weight 1 --ngpu 1 --ttsloss_weight 0 --spkloss_weight 0 --stage 4 --n_average 0 --stop_stage 4 2>&1 | tee log/run.onlynceloss_improve_reportnceval.shufflebatching.bs76.chunks.from200to400.stage4only.log
##### with less queue size
(1216 quesize) bash run.spkidonly_nceloss.sh --train_config conf/spkidonly_nceloss_improve_reportnceval_shufflebatching_bs76_K1216.yaml --nceloss_weight 1 --ngpu 1 --ttsloss_weight 0 --spkloss_weight 0 --stage 4 --n_average 0 --stop_stage 4 2>&1 | tee log/run.onlynceloss_improve_reportnceval.shufflebatching.bs76.K1216.chunks.from200to400.stage4only.log
(12160 quesize) bash run.spkidonly_nceloss.sh --train_config conf/spkidonly_nceloss_improve_reportnceval_shufflebatching_bs76_K12160.yaml --nceloss_weight 1 --ngpu 1 --ttsloss_weight 0 --spkloss_weight 0 --stage 4 --n_average 0 --stop_stage 4 2>&1 | tee log/run.onlynceloss_improve_reportnceval.shufflebatching.bs76.K12160.chunks.from200to400.stage4only.log



## TTS + SPKID + NCEloss
(ING (bs56. idk but bs64 n bs60 OOM), FT2 w/ nce_loss + specaug + shufflebatching, nceloss_weight=0.001, lr=1e-3) bash run.asrttsspkid.spkloss_weight.new.update.rf3.fullyunsync.addnceloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_specaug_bs56.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --nceloss_weight 0.001 2>&1 | tee log/run.asrttsspkid.spkloss_weight.new.update.rf3.fullyunsync.spkloss_weight0.03.nceloss_weight0.001.onlystage4.FT2_2flstm512Nusecat.shufflebatching.specaug.bs56.log # conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_specaug_bs56.yaml seems to change to conf/train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_specaug_bs56.yaml
(ING (bs56. idk but bs64 n bs60 OOM), FT2 w/ nce_loss + specaugfix2 + shufflebatching, nceloss_weight=0.001, lr=1e-3) bash run.asrttsspkid.spkloss_weight.new.update.rf3.fullyunsync.addnceloss.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_specaugfix2_bs56.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --nceloss_weight 0.001 2>&1 | tee log/run.asrttsspkid.spkloss_weight.new.update.rf3.fullyunsync.spkloss_weight0.03.nceloss_weight0.001.onlystage4.FT2_2flstm512Nusecat.shufflebatching.specaugfix2.bs56.log






# 2. Back-end
# 2-1. LDA + PLDA
# ICASSP 2021 exp.s (fullyunsync & shufflebatching) - start
## 1. Unsupervised learning - different architecture
### 2flstm512
#### NObceloss
(ING) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_train_pytorch_train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0 2>&1 | tee log/backend.voxceleb1_train_filtered_train_pytorch_train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0.currentlossbest.ep.23.log
#### extended to ep60 with lr=1e-4
(DONE: 5.13) bash run_backendonly_cpuparallel.voxceleb1.sh --model snapshot.ep.58 --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_train_pytorch_train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_lr1e-4_ep60_spkloss_weight0/ 2>&1 | tee log/backend.voxceleb1_train_filtered_train_pytorch_train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0.ext_ep60_lr1e-4.finallossbest.ep.58.log
### 2flstm1024
(DONE, currentlossbest (ep.23), 2flstm1024) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_train_pytorch_train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm1024Nusecat_fullyunsync_shufflebatching_spkloss_weight0 2>&1 | tee log/backend.voxceleb1_train_filtered_train_pytorch_train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm1024Nusecat_fullyunsync_shufflebatching_spkloss_weight0.currentlossbest.ep.23.log
(DONE: 5.14, finallossbest (ep.29 or 30), 2flstm1024) bash run_backendonly_cpuparallel.voxceleb1.sh --model snapshot.ep.29 --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_train_pytorch_train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm1024Nusecat_fullyunsync_shufflebatching_spkloss_weight0 2>&1 | tee log/backend.voxceleb1_train_filtered_train_pytorch_train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm1024Nusecat_fullyunsync_shufflebatching_spkloss_weight0.finallossbest.ep.29.log
#### extended to ep60 with lr=1e-4
(DONE: 4.87) bash run_backendonly_cpuparallel.voxceleb1.sh --model snapshot.ep.58 --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_train_pytorch_train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm1024Nusecat_fullyunsync_shufflebatching_lr1e-4_ep60_spkloss_weight0/ 2>&1 | tee log/backend.voxceleb1_train_filtered_train_pytorch_train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm1024Nusecat_fullyunsync_shufflebatching_spkloss_weight0.ext_ep60_lr1e-4.finallossbest.ep.58.log
### 2flstm2048
(DONE: 5.05, finallossbest (ep.29 or 30), 2flstm2048) bash run_backendonly_cpuparallel.voxceleb1.sh --model snapshot.ep.29 --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_train_pytorch_train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm2048Nusecat_fullyunsync_shufflebatching_spkloss_weight0 2>&1 | tee log/backend.voxceleb1_train_filtered_train_pytorch_train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm2048Nusecat_fullyunsync_shufflebatching_spkloss_weight0.finallossbest.ep.29.log
#### extended to ep60 with lr=1e-4
(DONE: 4.80) bash run_backendonly_cpuparallel.voxceleb1.sh --model snapshot.ep.57 --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_train_pytorch_train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm2048Nusecat_fullyunsync_shufflebatching_lr1e-4_ep60_spkloss_weight0/ 2>&1 | tee log/backend.voxceleb1_train_filtered_train_pytorch_train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm2048Nusecat_fullyunsync_shufflebatching_spkloss_weight0.ext_ep60_lr1e-4.finallossbest.ep.57.log
#### extended to ep75 with lr=1e-5
### 3flstm512
(DONE: 5.40, finallossbest, 3flstm512) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_train_pytorch_train_pytorch_forwardtacotron2+spkemb_noatt_rf1_3flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0 2>&1 | tee log/backend.voxceleb1_train_filtered_train_pytorch_train_pytorch_forwardtacotron2+spkemb_noatt_rf1_3flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0.finallossbest.log
### 4flstm512
(DONE: 5.43, finallossbest, 4flstm512) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_train_pytorch_train_pytorch_forwardtacotron2+spkemb_noatt_rf1_4flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0 2>&1 | tee log/backend.voxceleb1_train_filtered_train_pytorch_train_pytorch_forwardtacotron2+spkemb_noatt_rf1_4flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0.finallossbest.log
## 2. Unsupervised learning - NCE with spkembnorm in nceloss (+ margin like AM softmax loss)
#### Since file names were too long, B4 I run the below backend commands I run:
#### for ename in `ls -ltrd exp/* | grep spkembnorm | grep bs56 | awk '{print $NF}'`;do chename=`echo $ename | sed "s/train_filtered_train_pytorch_train_pytorch/train_filtered_pytorch/g" | sed "s/_fullyunsync$//g" | sed "s/_noatt_/_/g" | sed "s/_nceloss_fullyunsync_shufflebatching/_fullyunsync_shufflebatching/g" | sed "s/weight/w/g"`;mv $ename $chename; done
##### nceloss_w = 0.001
(ING, finallossbest, spkembnormNCEloss w/ 30N0.2 4 margin) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_pytorch_forwardtacotron2+spkemb_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_spkembnorm_amsoftmaxINnceloss30N0.2_bs56_spkloss_w0_nceloss_w0.001 2>&1 | tee log/backend.voxceleb1_train_filtered_pytorch_forwardtacotron2+spkemb_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_spkembnorm_amsoftmaxINnceloss30N0.2_bs56_spkloss_w0_nceloss_w0.001.finallossbest.log
##### nceloss_w = 0.01
(DONE: 5.57, finallossbest, spkembnormNCEloss) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_pytorch_forwardtacotron2+spkemb_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_spkembnorm_bs56_spkloss_w0_nceloss_w0.01 2>&1 | tee log/backend.voxceleb1_train_filtered_pytorch_forwardtacotron2+spkemb_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_spkembnorm_bs56_spkloss_w0_nceloss_w0.01.finallossbest.log
(ING, finallossbest, spkembnormNCEloss w/ 30N0.1 4 margin) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_pytorch_forwardtacotron2+spkemb_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_spkembnorm_amsoftmaxINnceloss30N0.1_bs56_spkloss_w0_nceloss_w0.01 2>&1 | tee log/backend.voxceleb1_train_filtered_pytorch_forwardtacotron2+spkemb_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_spkembnorm_amsoftmaxINnceloss30N0.1_bs56_spkloss_w0_nceloss_w0.01.finallossbest.log
(DONE: 5.68, finallossbest, spkembnormNCEloss w/ 30N0.2 4 margin) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_pytorch_forwardtacotron2+spkemb_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_spkembnorm_amsoftmaxINnceloss30N0.2_bs56_spkloss_w0_nceloss_w0.01 2>&1 | tee log/backend.voxceleb1_train_filtered_pytorch_forwardtacotron2+spkemb_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_spkembnorm_amsoftmaxINnceloss30N0.2_bs56_spkloss_w0_nceloss_w0.01.finallossbest.log
(DONE: 5.98, finallossbest, spkembnormNCEloss w/ 30N0.3 4 margin) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_pytorch_forwardtacotron2+spkemb_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_spkembnorm_amsoftmaxINnceloss30N0.3_bs56_spkloss_w0_nceloss_w0.01 2>&1 | tee log/backend.voxceleb1_train_filtered_pytorch_forwardtacotron2+spkemb_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_spkembnorm_amsoftmaxINnceloss30N0.3_bs56_spkloss_w0_nceloss_w0.01.finallossbest.log
(DONE: 6.41, finallossbest, spkembnormNCEloss w/ 60N0.2 4 margin) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_pytorch_forwardtacotron2+spkemb_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_spkembnorm_amsoftmaxINnceloss60N0.2_bs56_spkloss_w0_nceloss_w0.01 2>&1 | tee log/backend.voxceleb1_train_filtered_pytorch_forwardtacotron2+spkemb_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_spkembnorm_amsoftmaxINnceloss60N0.2_bs56_spkloss_w0_nceloss_w0.01.finallossbest.log
(DONE: 7.06, finallossbest, spkembnormNCEloss w/ 60N0.3 4 margin) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_pytorch_forwardtacotron2+spkemb_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_spkembnorm_amsoftmaxINnceloss60N0.3_bs56_spkloss_w0_nceloss_w0.01 2>&1 | tee log/backend.voxceleb1_train_filtered_pytorch_forwardtacotron2+spkemb_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_spkembnorm_amsoftmaxINnceloss60N0.3_bs56_spkloss_w0_nceloss_w0.01.finallossbest.log
##### nceloss_w = 0.1
(ING, finallossbest, spkembnormNCEloss) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_pytorch_forwardtacotron2+spkemb_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_spkembnorm_bs56_spkloss_w0_nceloss_w0.1 2>&1 | tee log/backend.voxceleb1_train_filtered_pytorch_forwardtacotron2+spkemb_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_spkembnorm_bs56_spkloss_w0_nceloss_w0.1.finallossbest.log
(ING, finallossbest, spkembnormNCEloss w/ 30N0.2 4 margin) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_pytorch_forwardtacotron2+spkemb_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_spkembnorm_amsoftmaxINnceloss30N0.2_bs56_spkloss_w0_nceloss_w0.1 2>&1 | tee log/backend.voxceleb1_train_filtered_pytorch_forwardtacotron2+spkemb_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_spkembnorm_amsoftmaxINnceloss30N0.2_bs56_spkloss_w0_nceloss_w0.1.finallossbest.log
(ING, finallossbest, spkembnormNCEloss w/ 1N0.2 4 margin) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_pytorch_forwardtacotron2+spkemb_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_spkembnorm_amsoftmaxINnceloss1N0.2_bs56_spkloss_w0_nceloss_w0.1 2>&1 | tee log/backend.voxceleb1_train_filtered_pytorch_forwardtacotron2+spkemb_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_spkembnorm_amsoftmaxINnceloss1N0.2_bs56_spkloss_w0_nceloss_w0.1.finallossbest.log
##### nceloss_w = 1
(ING, finallossbest, spkembnormNCEloss w/ 30N0.2 4 margin) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_pytorch_forwardtacotron2+spkemb_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_spkembnorm_amsoftmaxINnceloss30N0.2_bs56_spkloss_w0_nceloss_w1 2>&1 | tee log/backend.voxceleb1_train_filtered_pytorch_forwardtacotron2+spkemb_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_spkembnorm_amsoftmaxINnceloss30N0.2_bs56_spkloss_w0_nceloss_w1.finallossbest.log
(ING, finallossbest, spkembnormNCEloss w/ 1N0.2 4 margin) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_pytorch_forwardtacotron2+spkemb_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_spkembnorm_amsoftmaxINnceloss1N0.2_bs56_spkloss_w0_nceloss_w1 2>&1 | tee log/backend.voxceleb1_train_filtered_pytorch_forwardtacotron2+spkemb_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_spkembnorm_amsoftmaxINnceloss1N0.2_bs56_spkloss_w0_nceloss_w1.finallossbest.log

### (TODO: Different weights for NCE loss)
# ICASSP 2021 exp.s - end
# *** example (with the lastest code as of 20200902) - start
## spkid-tts (ep.29 is not the best in any val. metrics but in EER results, this is the best as 3.44)
### vanila
(* DONE: 3.54 (alllossesbest as ep.28), 2flstm512 & use_concate=true) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_train_pytorch_train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0.03 2>&1 | tee log/backend.voxceleb1_train_filtered_train_pytorch_train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0.03.log
### + l1 consloss
(DONE) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_train_pytorch_train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_spkloss_weight0.03_consloss_weight0.1 2>&1 | tee log/backend.voxceleb1_train_filtered_train_pytorch_train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0.03.consloss_w0.1.log
(DONE) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_train_pytorch_train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_spkloss_weight0.03_consloss_weight0.01 2>&1 | tee log/backend.voxceleb1_train_filtered_train_pytorch_train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0.03.consloss_w0.01.log
### + l1 consloss normspkemb (12 exps)
long1=exp/voxceleb1_train_filtered_train_pytorch_train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_detach_ref_normspkemb_spkloss_weight0.03_consloss_weight0.01
short1=exp/voxceleb1_train_filtered_train_pytorch_ftacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_detach_ref_normspkemb_spkloss_weight0.03_consloss_weight0.01
mv $long1 $short1
long2=exp/voxceleb1_train_filtered_train_pytorch_train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_detach_syn_normspkemb_spkloss_weight0.03_consloss_weight0.01
short2=exp/voxceleb1_train_filtered_train_pytorch_ftacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_detach_syn_normspkemb_spkloss_weight0.03_consloss_weight0.01
mv $long2 $short2
#### 2 exps (the original exp names were too long)
for ename in $short1 $short2;do
    bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir ${ename} 2>&1 | tee log/backend.$(basename $ename).log &
done
#### rest 10 exps
for ename in `ls -d exp/voxceleb1_train_filtered_* | grep voxceleb1_train_filtered_train_pytorch_train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss | grep normspkemb`;do
    bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir ${ename} 2>&1 | tee log/backend.$(basename $ename).log &
done
### + cosdis loss (12 exps)
#### change expdir names (since they could be too long)
for ename in `ls -d exp/voxceleb1_train_filtered_* | grep voxceleb1_train_filtered_train_pytorch_train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss | grep cosdis | grep -v debug_stopaterror`;do
    ename_short=`echo $ename | sed -e 's/pytorch_train_pytorch_//g'`
    mv $ename $ename_short
done
#### actual backend run
for ename in `ls -d exp/voxceleb1_train_filtered_* | grep voxceleb1_train_filtered_train_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss | grep cosdis | grep -v debug_stopaterror`;do
    bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir ${ename} 2>&1 | tee log/backend.$(basename $ename).log &
done
#### var lda_dim for
##### 1. exp/voxceleb1_train_filtered_train_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_cosdis_detach_ref_spkloss_weight0.03_consloss_weight0.01
ename=exp/voxceleb1_train_filtered_train_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_cosdis_detach_ref_spkloss_weight0.03_consloss_weight0.01
for lda_dim in 150 200 250 300 350 400;do
    (bash run_backendonly_cpuparallel.varlda_dim.voxceleb1.sh --lda_dim ${lda_dim} --stage 6 --nj 70 --ngpu 0 --expdir ${ename} 2>&1 | tee log/backend.lda_dim${lda_dim}.$(basename $ename).log) &
done
##### 2. exp/voxceleb1_train_filtered_train_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_cosdis_detach_ref_spkloss_weight0.03_consloss_weight0.1
ename=exp/voxceleb1_train_filtered_train_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_cosdis_detach_ref_spkloss_weight0.03_consloss_weight0.1
for lda_dim in 150 200 250 300 350 400;do
    (bash run_backendonly_cpuparallel.varlda_dim.voxceleb1.sh --lda_dim ${lda_dim} --stage 6 --nj 70 --ngpu 0 --expdir ${ename} 2>&1 | tee log/backend.lda_dim${lda_dim}.$(basename $ename).log) &
done

### semi-supervised (a speaker embedding extractor trained with only voxceleb2 data) - *** features are normalized with data/voxceleb1_train_filtered_train/cmvn.ark (NOT with voxceleb2 data. However, I assume the cmvn stats are similar) ***
#### 800spk
##### 800spk lab only
(DONE: 3.93) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_800spk_stack_lab_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0.03_fullyunsync 2>&1 | tee log/backend.voxceleb2_800spk_stack_lab_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0.03_fullyunsync.lossbest.log
(DONE: 3.89 for ICASSP 2021, finallossbest) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --model snapshot.ep.32 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_800spk_stack_lab_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_lr1e-4_ep60_spkloss_weight0.03_fullyunsync/ 2>&1 | tee log/backend.voxceleb2_800spk_stack_lab_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_lr1e-4_ep60_spkloss_weight0.03_fullyunsync.finallossbest.ep.32.log
(DONE: 3.94 for ICASSP 2021, finall1lossbest) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --model snapshot.ep.52 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_800spk_stack_lab_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_lr1e-4_ep60_spkloss_weight0.03_fullyunsync/ 2>&1 | tee log/backend.voxceleb2_800spk_stack_lab_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_lr1e-4_ep60_spkloss_weight0.03_fullyunsync.finall1lossbest.ep.52.log
(DONE: 3.96 for ICASSP 2021, finalmselossbest) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --model snapshot.ep.48 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_800spk_stack_lab_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_lr1e-4_ep60_spkloss_weight0.03_fullyunsync/ 2>&1 | tee log/backend.voxceleb2_800spk_stack_lab_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_lr1e-4_ep60_spkloss_weight0.03_fullyunsync.finalmselossbest.ep.48.log
(ING)
for lda_dim in 150 200 250 300 350 400;do
    (bash run_backendonly_cpuparallel.varlda_dim.voxceleb1.sh --lda_dim ${lda_dim} --stage 6 --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_800spk_stack_lab_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0.03_fullyunsync 2>&1 | tee log/backend.lda_dim${lda_dim}.voxceleb2_800spk_stack_lab_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0.03_fullyunsync.losbest.log ) &
done
##### 800spk lab + libritts unlab
# (first I changed the expname since it was too long) mv exp/voxceleb2_800spk_stack_lab_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_semi_labfromvox2_nolabfromlibritts_spkloss_weight0.03_fullyunsync_labfromvox2_nolabfromlibritts/ exp/voxceleb2_800spk_lab_libritts_nolab_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_semi_spkloss_weight0.03/
bash run_backendonly_cpuparallel.voxceleb1.sh --model snapshot.ep.27 --nj 70 --ngpu 0 --expdir /export/b18/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_800spk_lab_libritts_nolab_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_semi_spkloss_weight0.03/ 2>&1 | tee log/backend.voxceleb2_800spk_lab_libritts_nolab_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_semi_spkloss_weight0.03.finallossbest.ep.27.log
##### 800spk lab + 800 spk unlab
(DONE: 4.18, finallossbest, 2flstm512 & use_concate=true) bash run_backendonly_cpuparallel.voxceleb1.sh --model snapshot.ep.14 --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_800spk_stack_lab_train_800nolabspk_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_semi_spkloss_weight0.03_fullyunsync 2>&1 | tee log/backend.voxceleb2_800spk_stack_lab_train_800nolabspk_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_semi_spkloss_weight0.03_fullyunsync.finallossbest.ep.14.log
(DONE: 3.82, finalreconlossbest, 2flstm512 & use_concate=true) bash run_backendonly_cpuparallel.voxceleb1.sh --model snapshot.ep.28 --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_800spk_stack_lab_train_800nolabspk_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_semi_spkloss_weight0.03_fullyunsync 2>&1 | tee log/backend.voxceleb2_800spk_stack_lab_train_800nolabspk_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_semi_spkloss_weight0.03_fullyunsync.finalreconlossbest.ep.28.log
###### extended to lr=1e-4 and spkloss_weight=0.003 to ep45
(DONE:3.84, reasonable picked manually, 2flstm512 & use_concate=true) bash run_backendonly_cpuparallel.voxceleb1.sh --model snapshot.ep.29 --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_800spk_stack_lab_train_800nolabspk_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_semi_lr1e-4_ep45_spkloss_weight0.003_fullyunsync/ 2>&1 | tee log/backend.voxceleb2_800spk_stack_lab_train_800nolabspk_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_semi_tolr1e-4Nep45_spkloss_weight0.03to0.003_fullyunsync.resonablelossselectedmanually.ep.29.log

###### (+) nceloss
# (First change ename to a shorter one) mv /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_800spk_stack_lab_train_800nolabspk_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_nceloss_semi_bs56_NOspklosswWeightingbyRatio_spkloss_weight0.03_fullyunsync/ /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_800spk_lab_800nolabspk_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_nceloss_semi_bs56_NOspklosswWeightingbyRatio_spkloss_weight0.03/
(ING, currentlossbest, 2flstm512 & use_concate=true) bash run_backendonly_cpuparallel.voxceleb1.sh --model snapshot.ep.21 --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_800spk_lab_800nolabspk_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_nceloss_semi_bs56_NOspklosswWeightingbyRatio_spkloss_weight0.03/ 2>&1 | tee log/backend.voxceleb2_800spk_lab_800nolabspk_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_nceloss_semi_bs56_NOspklosswWeightingbyRatio_spkloss_weight0.03.currentlossbest.ep.21.log
##### 800spk lab + 1600 spk unlab
(DONE: 3.82, finalreconlossbest, 2flstm512 & use_concate=true) bash run_backendonly_cpuparallel.voxceleb1.sh --model snapshot.ep.18 --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_800spk_stack_lab_train_1600nolabspk_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_semi_spkloss_weight0.03_fullyunsync 2>&1 | tee log/backend.voxceleb2_800spk_stack_lab_train_1600nolabspk_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_semi_spkloss_weight0.03_fullyunsync.finallossbest.ep.18.log
(DONE: 3.92, finallossbest, 2flstm512 & use_concate=true) bash run_backendonly_cpuparallel.voxceleb1.sh --model snapshot.ep.26 --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_800spk_stack_lab_train_1600nolabspk_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_semi_spkloss_weight0.03_fullyunsync 2>&1 | tee log/backend.voxceleb2_800spk_stack_lab_train_1600nolabspk_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_semi_spkloss_weight0.03_fullyunsync.finallossbest.ep.26.log
###### extended to lr=1e-4 and spkloss_weight=0.003
(DONE: 3.73, reasonable picked manually, 2flstm512 & use_concate=true) bash run_backendonly_cpuparallel.voxceleb1.sh --model snapshot.ep.19 --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_800spk_stack_lab_train_1600nolabspk_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_semi_lr1e-4_spkloss_weight0.003_fullyunsync/ 2>&1 | tee log/backend.voxceleb2_800spk_stack_lab_train_1600nolabspk_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_semi_tolr1e-4_spkloss_weight0.03to0.003_fullyunsync.resonablelossselectedmanually.ep.19.log
##### 800spk lab + 3200 spk unlab
(DONE: 3.64, currentlossbest, 2flstm512 & use_concate=true) bash run_backendonly_cpuparallel.voxceleb1.sh --model snapshot.ep.9 --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_800spk_stack_lab_train_3200nolabspk_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_semi_spkloss_weight0.03_fullyunsync 2>&1 | tee log/backend.voxceleb2_800spk_stack_lab_train_3200nolabspk_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_semi_spkloss_weight0.03_fullyunsync.finallossbest.ep.9.log
(DONE: 3.83, current2ndlossbest but in much later epochs, 2flstm512 & use_concate=true) bash run_backendonly_cpuparallel.voxceleb1.sh --model snapshot.ep.23 --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_800spk_stack_lab_train_3200nolabspk_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_semi_spkloss_weight0.03_fullyunsync 2>&1 | tee log/backend.voxceleb2_800spk_stack_lab_train_3200nolabspk_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_semi_spkloss_weight0.03_fullyunsync.final2ndlossbest.ep.23.log

##### 800spk lab + rest unlab
(DONE: 4.06 current lossbest, 2flstm512 & use_concate=true) bash run_backendonly_cpuparallel.voxceleb1.sh --model snapshot.ep.3 --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_800spk_stack_lab_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_semi_spkloss_weight0.03_fullyunsync 2>&1 | tee log/backend.voxceleb2_800spk_stack_lab_train_2flstm512Nusecat_fullyunsync_shufflebatching_semi_spkloss_weight0.03.currentlossbest.ep.3.log
(DONE: 3.93: current mselossbest, 2flstm512 & use_concate=true) bash run_backendonly_cpuparallel.voxceleb1.sh --model snapshot.ep.4 --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_800spk_stack_lab_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_semi_spkloss_weight0.03_fullyunsync 2>&1 | tee log/backend.voxceleb2_800spk_stack_lab_train_2flstm512Nusecat_fullyunsync_shufflebatching_semi_spkloss_weight0.03.currentmselossbest.ep.4.log
(ING)
for lda_dim in 150 200 250 300 350 400;do
    (bash run_backendonly_cpuparallel.varlda_dim.voxceleb1.sh --lda_dim ${lda_dim} --model snapshot.ep.4 --stage 6 --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_800spk_stack_lab_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_semi_spkloss_weight0.03_fullyunsync 2>&1 | tee log/backend.lda_dim${lda_dim}.voxceleb2_800spk_stack_lab_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_semi_spkloss_weight0.03_fullyunsync.currentmselossbest.ep.4.log ) &
done
(DONE: 4.04: current spkidaccbest, 2flstm512 & use_concate=true) bash run_backendonly_cpuparallel.voxceleb1.sh --model snapshot.ep.7 --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_800spk_stack_lab_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_semi_spkloss_weight0.03_fullyunsync 2>&1 | tee log/backend.voxceleb2_800spk_stack_lab_train_2flstm512Nusecat_fullyunsync_shufflebatching_semi_spkloss_weight0.03.currentspkidaccbest.ep.7.log
###### change lr from lr=1e-3 & spkloss_weight=0.03 to the lower
####### to lr=1e-4 & spkloss_weight=0.003
(DONE: 3.47, current lossbest, 2flstm512 & use_concate=true) bash run_backendonly_cpuparallel.voxceleb1.sh --model snapshot.ep.18 --nj 70 --ngpu 0 --expdir /export/b18/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_800spk_stack_lab_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_semi_lr1e-4_spkloss_weight0.003_fullyunsync/ 2>&1 | tee log/backend.voxceleb2_800spk_stack_lab_train_2flstm512Nusecat_fullyunsync_shufflebatching_semi_spkloss_weightfrom0.03to0.003_lrfrom1e-3to1e-4.currentlossbest.ep.18.log
#### 1600spk
##### 1600spk + rest unlab
(DONE, current lossbest: ep.15) bash run_backendonly_cpuparallel.voxceleb1.sh --model snapshot.ep.15 --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_1600spk_stack_lab_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_semi_spkloss_weight0.03_fullyunsync 2>&1 | tee log/backend.voxceleb2_1600spk_stack_lab_train_2flstm512Nusecat_fullyunsync_shufflebatching_semi_spkloss_weight0.03.currentlossbest.ep.15.log
(ING, lossbestAmong30: ep.19) bash run_backendonly_cpuparallel.voxceleb1.sh --model snapshot.ep.19 --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_1600spk_stack_lab_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_semi_spkloss_weight0.03_fullyunsync 2>&1 | tee log/backend.voxceleb2_1600spk_stack_lab_train_2flstm512Nusecat_fullyunsync_shufflebatching_semi_spkloss_weight0.03.lossbestAmong30.ep.19.log
#### 2400spk
##### 2400spk lab only
(DONE 2.67:, bestlossNspkidacc, ep.21) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_2400spk_stack_lab_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0.03_fullyunsync 2>&1 | tee log/backend.voxceleb2_2400spk_stack_lab_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0.03_fullyunsync.bestlossNspkidacc.ep.21.log
expdir=/export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_2400spk_stack_lab_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0.03_fullyunsync
model=''
model_selection=bestlossNspkidacc
for lda_dim in 150 200 250 300 350 400;do
    if [[ ${model} == *".ep."* ]]; then
        (bash run_backendonly_cpuparallel.varlda_dim.voxceleb1.sh --lda_dim ${lda_dim} --model ${model} --stage 6 --nj 70 --ngpu 0 --expdir ${expdir} 2>&1 | tee log/backend.lda_dim${lda_dim}.$(basename ${expdir}).${model_selection}.${model#snapshot.}.log) &
    else
        (bash run_backendonly_cpuparallel.varlda_dim.voxceleb1.sh --lda_dim ${lda_dim} --stage 6 --nj 70 --ngpu 0 --expdir ${expdir} 2>&1 | tee log/backend.lda_dim${lda_dim}.$(basename ${expdir}).${model_selection}.log) &
    fi
done
(ING, bestreconlosses, ep.26) bash run_backendonly_cpuparallel.voxceleb1.sh --model snapshot.ep.26 --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_2400spk_stack_lab_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0.03_fullyunsync 2>&1 | tee log/backend.voxceleb2_2400spk_stack_lab_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0.03_fullyunsync.bestreconlosses.ep.26.log
##### 2400spk lab + rest unlab
(DONE, current lossbest: ep.15) bash run_backendonly_cpuparallel.voxceleb1.sh --model snapshot.ep.15 --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_2400spk_stack_lab_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_semi_spkloss_weight0.03_fullyunsync 2>&1 | tee log/backend.voxceleb2_2400spk_stack_lab_train_2flstm512Nusecat_fullyunsync_shufflebatching_semi_spkloss_weight0.03.currentlossbest.ep.15.log
(DONE: 2.46, lossbestAmong30: ep.25) bash run_backendonly_cpuparallel.voxceleb1.sh --model snapshot.ep.25 --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_2400spk_stack_lab_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_semi_spkloss_weight0.03_fullyunsync 2>&1 | tee log/backend.voxceleb2_2400spk_stack_lab_train_2flstm512Nusecat_fullyunsync_shufflebatching_semi_spkloss_weight0.03.lossbestAmong30.ep.25.log
(DONE: 2.36 with lda_dim=200)
for lda_dim in 150 200 250 300 350 400; do
    (bash run_backendonly_cpuparallel.varlda_dim.voxceleb1.sh --lda_dim ${lda_dim} --model snapshot.ep.25 --stage 6 --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_2400spk_stack_lab_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_semi_spkloss_weight0.03_fullyunsync 2>&1 | tee log/backend.lda_dim${lda_dim}.voxceleb2_2400spk_stack_lab_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_semi_spkloss_weight0.03_fullyunsync.currentmselossbest.ep.25.log ) &
#### 3200spk (stopped for the other job and now running again but later than other *spk)
##### 3200spk + rest unlab
(ING, lossbestAmong30: ep.6) bash run_backendonly_cpuparallel.voxceleb1.sh --model snapshot.ep.6 --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_3200spk_stack_lab_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_semi_spkloss_weight0.03_fullyunsync 2>&1 | tee log/backend.voxceleb2_3200spk_stack_lab_train_2flstm512Nusecat_fullyunsync_shufflebatching_semi_spkloss_weight0.03.lossbestAmong30.ep.6.log
(ING, loss2ndbestAmong30: ep.26) bash run_backendonly_cpuparallel.voxceleb1.sh --model snapshot.ep.26 --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_3200spk_stack_lab_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_semi_spkloss_weight0.03_fullyunsync 2>&1 | tee log/backend.voxceleb2_3200spk_stack_lab_train_2flstm512Nusecat_fullyunsync_shufflebatching_semi_spkloss_weight0.03.lossbestAmong30.ep.26.log
#### 4000spk
##### 4000spk + rest unlab
(DONE, current lossbest: ep.14) bash run_backendonly_cpuparallel.voxceleb1.sh --model snapshot.ep.14 --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_4000spk_stack_lab_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_semi_spkloss_weight0.03_fullyunsync 2>&1 | tee log/backend.voxceleb2_4000spk_stack_lab_train_2flstm512Nusecat_fullyunsync_shufflebatching_semi_spkloss_weight0.03.currentlossbest.ep.14.log
(DONE: lossbestAmong30 is the same as the above)
#### 6114spk (unsupservised)
(DONE for ICASSP 2021, finallossbest: ep.29) bash run_backendonly_cpuparallel.voxceleb1.sh --model snapshot.ep.29 --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0_fullyunsync 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0_fullyunsync.finallossbest.ep.29.log
(DONE with cosine scoring) bash run_cosbackend_cpuparallel.voxceleb1.sh --stage 7 --model snapshot.ep.29 --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0_fullyunsync 2>&1 | tee log/backend_cs.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0_fullyunsync.finallossbest.ep.29.log
(ING with spkembnorm) bash run_backendonly_cpuparallel.voxceleb1.sh --model snapshot.ep.24 --stage 6 --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkembnorm_spkloss_weight0_fullyunsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkembnorm_spkloss_weight0_fullyunsync.finallossbest.ep.24.log
## spkid-tts + nceloss
(* DONE: 3.45 checkafterrefix spkidaccbest (ep.28), 2flstm512 & use_concate=true, lr1 + shufflebatching, nceloss=0.001, bs56) bash run_backendonly_cpuparallel.voxceleb1.sh --model snapshot.ep.28 --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_train_pytorch_forwardtacotron2+spkemb_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_bs56_checkafterrefix_spkloss_weight0.03_nceloss_weight0.001/ 2>&1 | tee log/backend.voxceleb1_train_filtered_train_pytorch_forwardtacotron2+spkemb_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_bs56_checkafterrefix_spkloss0.03_nceloss0.001.spkaccbest.snapshot.ep.28.log
# *** example - end
## spkid-tts wth simple softmax
(DONE: 4.52, spkloss_weight 0.01 w/ softmax) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_train_pytorch_train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_softmax_spkloss_weight0.01/ 2>&1 | tee log/backend.voxceleb1_train_filtered_train_pytorch_train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_softmax_spkloss_weight0.01.log
(DONE: 4.10 (lossNreconlossesbest as ep.28), spkloss_weight 0.03 w/ softmax) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_train_pytorch_train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_softmax_spkloss_weight0.03/ 2>&1 | tee log/backend.voxceleb1_train_filtered_train_pytorch_train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_softmax_spkloss_weight0.03.log
(DONE: 4.39 (spkid{acc,loss}best as ep.21 quite large difference compared to the ep.28), spkloss_weight 0.03 w/ softmax) bash run_backendonly_cpuparallel.voxceleb1.sh --model snapshot.ep.21 --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_train_pytorch_train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_softmax_spkloss_weight0.03/ 2>&1 | tee log/backend.voxceleb1_train_filtered_train_pytorch_train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_softmax_spkloss_weight0.03.spkidaccNspkidlossbest.ep.21.log
(DONE: 4.87, spkloss_weight 0.1 w/ softmax) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_train_pytorch_train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_softmax_spkloss_weight0.1/ 2>&1 | tee log/backend.voxceleb1_train_filtered_train_pytorch_train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_softmax_spkloss_weight0.1.log
## spkid only
### whole utt. training
(DONE: 4.94, ep.10 bestspkidloss (loss starts after the ep.) among 50 eps, same features with TTS (+SPKID) exp.s) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_train_pytorch_train_pytorch_tacotron2+spkemb_spkidonly_speakeridonly_ttslossw0_spkloss_weight1_final/ 2>&1 | tee log/backend.voxceleb1_train_filtered_train_pytorch_train_pytorch_tacotron2+spkemb_spkidonly_speakeridonly_ttslossw0_spkloss_weight1_final.bestspkidloss.ep.10.log
(DONE: 4.84, ep.41 bestspkidacc among 50 eps, same features with TTS (+SPKID) exp.s) bash run_backendonly_cpuparallel.voxceleb1.sh --model snapshot.ep.41 --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_train_pytorch_train_pytorch_tacotron2+spkemb_spkidonly_speakeridonly_ttslossw0_spkloss_weight1_final/ 2>&1 | tee log/backend.voxceleb1_train_filtered_train_pytorch_train_pytorch_tacotron2+spkemb_spkidonly_speakeridonly_ttslossw0_spkloss_weight1_final.bestspkidacc.ep.41.log
#### various lda_dim
(DONE: 4.74 is the best (EER) with lda_dim=250)
for lda_dim in 150 200 250 300 350 400;do
    (bash run_backendonly_cpuparallel.varlda_dim.voxceleb1.sh --lda_dim ${lda_dim} --model snapshot.ep.41 --stage 6 --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_train_pytorch_train_pytorch_tacotron2+spkemb_spkidonly_speakeridonly_ttslossw0_spkloss_weight1_final/ 2>&1 | tee log/backend.lda_dim${lda_dim}.voxceleb1_train_filtered_train_pytorch_train_pytorch_tacotron2+spkemb_spkidonly_speakeridonly_ttslossw0_spkloss_weight1_final.bestspkidacc.ep.41.log ) &
done
### chunk training
(DONE: 4.01, ep.48 bestspkidlossNspkidacc among 50 eps, same features with TTS (+SPKID) exp.s) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_train_pytorch_train_pytorch_tacotron2+spkemb_spkidonly_shufflebatching_bs128_speakeridonly_ttslossw0_spkloss_weight1_final_unsync200to400random/ 2>&1 | tee log/backend.voxceleb1_train_filtered_train_pytorch_train_pytorch_tacotron2+spkemb_spkidonly_shufflebatching_bs128_speakeridonly_ttslossw0_spkloss_weight1_final_unsync200to400random.bestspkidlossNspkidacc.ep.48.log
(DONE: 3.93, ep.59 bestspkidloss among 80 eps, same features with TTS (+SPKID) exp.s) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_train_pytorch_train_pytorch_tacotron2+spkemb_spkidonly_shufflebatching_bs128_lr1e-4_ep80_speakeridonly_ttslossw0_spkloss_weight1_final_unsync200to400random/ 2>&1 | tee log/backend.voxceleb1_train_filtered_train_pytorch_train_pytorch_tacotron2+spkemb_spkidonly_shufflebatching_bs128_lr1e-4_ep80_speakeridonly_ttslossw0_spkloss_weight1_final_unsync200to400random.bestspkidloss.ep.59.log
(DONE: 3.92, ep.74 bestspkidaccN2ndbestspkidloss among 80 eps, same features with TTS (+SPKID) exp.s) bash run_backendonly_cpuparallel.voxceleb1.sh --model snapshot.ep.74 --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_train_pytorch_train_pytorch_tacotron2+spkemb_spkidonly_shufflebatching_bs128_lr1e-4_ep80_speakeridonly_ttslossw0_spkloss_weight1_final_unsync200to400random/ 2>&1 | tee log/backend.voxceleb1_train_filtered_train_pytorch_train_pytorch_tacotron2+spkemb_spkidonly_shufflebatching_bs128_lr1e-4_ep80_speakeridonly_ttslossw0_spkloss_weight1_final_unsync200to400random.bestspkidaccN2ndbestspkidloss.ep.74.log
(* DONE: 3.89, ep.94 bestspkidloss among 110 eps, same features with TTS (+SPKID) exp.s) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_train_pytorch_train_pytorch_tacotron2+spkemb_spkidonly_shufflebatching_bs128_lr1e-5_ep110_speakeridonly_ttslossw0_spkloss_weight1_final_unsync200to400random/ 2>&1 | tee log/backend.exp/voxceleb1_train_filtered_train_pytorch_train_pytorch_tacotron2+spkemb_spkidonly_shufflebatching_bs128_lr1e-5_ep110_speakeridonly_ttslossw0_spkloss_weight1_final_unsync200to400random.bestspkidloss.log
#### add 3-dim pitch
(DONE: 3.81, spklossbest, ep.49) bash run_backendonly_cpuparallel.voxceleb1.3pitchadd.sh --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_train_3pitchadd_pytorch_train_pytorch_tacotron2+spkemb_spkidonly_shufflebatching_bs128_3pitchadd_speakeridonly_ttslossw0_spkloss_weight1_final_unsync200to400random/ 2>&1 | tee log/backend.voxceleb1_train_filtered_train_3pitchadd_pytorch_train_pytorch_tacotron2+spkemb_spkidonly_shufflebatching_bs128_3pitchadd_speakeridonly_ttslossw0_spkloss_weight1_final_unsync200to400random.spklossbest.ep.49.log
## spkid only (+) nceloss
### * Now exp/voxceleb1_train_filtered_train_pytorch_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76_speakeridonly_ttslossw0_spkloss_weight1_final_nceloss_weight0.03_unsync200to400random moved to exp/voxceleb1_train_filtered_train_pytorch_spkidonly_shufflebatching_bs76_nceloss_weight0.03_unsync200to400random. And for all related files (e.g., the embedding file name & plda, etc.) too)
(DONE: 3.76, bestspkidloss) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_train_pytorch_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76_speakeridonly_ttslossw0_spkloss_weight1_final_nceloss_weight0.003_unsync200to400random 2>&1 | tee log/backend.voxceleb1_train_filtered_train_pytorch_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76_speakeridonly_ttslossw0_spkloss_weight1_final_nceloss_weight0.003_unsync200to400random.bestspkidloss.log
(DONE: 3.60, bestspkidloss) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_train_pytorch_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76_speakeridonly_ttslossw0_spkloss_weight1_final_nceloss_weight0.01_unsync200to400random 2>&1 | tee log/backend.voxceleb1_train_filtered_train_pytorch_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76_speakeridonly_ttslossw0_spkloss_weight1_final_nceloss_weight0.01_unsync200to400random.bestspkidloss.log
(* DONE: 3.82, bestspkidloss) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_train_pytorch_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76_speakeridonly_ttslossw0_spkloss_weight1_final_nceloss_weight0.03_unsync200to400random 2>&1 | tee log/backend.voxceleb1_train_filtered_train_pytorch_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76_speakeridonly_ttslossw0_spkloss_weight1_final_nceloss_weight0.03_unsync200to400random.bestspkidloss.log
(DONE: 3.93, bestspkidloss) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_train_pytorch_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76_speakeridonly_ttslossw0_spkloss_weight1_final_nceloss_weight0.1_unsync200to400random 2>&1 | tee log/backend.voxceleb1_train_filtered_train_pytorch_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76_speakeridonly_ttslossw0_spkloss_weight1_final_nceloss_weight0.1_unsync200to400random.bestspkidloss.log
(DONE: 3.99, bestspkidloss) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_train_pytorch_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76_speakeridonly_ttslossw0_spkloss_weight1_final_nceloss_weight0.3_unsync200to400random 2>&1 | tee log/backend.voxceleb1_train_filtered_train_pytorch_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76_speakeridonly_ttslossw0_spkloss_weight1_final_nceloss_weight0.3_unsync200to400random.bestspkidloss.log
### voxceleb2(Naug) front-end training (a speaker embedding extractor trained with only voxceleb2(Naug) data) - *** features are normalized with data/voxceleb1_train_filtered_train/cmvn.ark (NOT with voxceleb2(Naug) data. However, I assume the cmvn stats are similar) ***
#### tts only
(DONE: 4.07) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0_fullyunsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0_fullyunsync.bestalllosses.log
(DONE: 4.33) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56_spkloss_weight0_nceloss_weight0.001_fullyunsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56_spkloss_weight0_nceloss_weight0.001_fullyunsync.bestloss.log

#### spkid only + nce
##### voxceleb2
(DONE: 2.00) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_train_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76_spkloss_weight1_nceloss_weight0.1_unsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76_spkloss_weight1_nceloss_weight0.1_unsync.lossbest.log
##### different lda_dim
(DONE: 1.78 with lda_dim=300)
for lda_dim in 150 200 250 300 350 400;do
    (bash run_backendonly_cpuparallel.varlda_dim.voxceleb1.sh --lda_dim ${lda_dim} --stage 6 --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_train_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76_spkloss_weight1_nceloss_weight0.1_unsync/ 2>&1 | tee log/backend.lda_dim${lda_dim}.voxceleb2_train_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76_spkloss_weight1_nceloss_weight0.1_unsync.lossbest.log) &
done
##### voxceleb2Naug
###### nceloss=0.03
####### current lossbest
(DONE: 1.96, current spklossbest) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir /export/b15/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2Naug_train_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76_spkloss_weight1_nceloss_weight0.03_unsync 2>&1 | tee log/backend.voxceleb2Naug_train_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76_spkloss_weight1_nceloss_weight0.03_unsync.spklossbest.log
(DONE: 1.86 with lda_dim=250)
expdir=/export/b15/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2Naug_train_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76_spkloss_weight1_nceloss_weight0.03_unsync
model=currentspklossbest
for lda_dim in 150 200 250 300 350 400;do
    (bash run_backendonly_cpuparallel.varlda_dim.voxceleb1.sh --lda_dim ${lda_dim} --stage 6 --nj 70 --ngpu 0 --expdir ${expdir} 2>&1 | tee log/backend.lda_dim${lda_dim}.$(basename ${expdir}).${model}.log) &
done
###### nceloss=0.1
(DONE: 2.01, current spklossbest) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir /export/b15/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2Naug_train_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76_spkloss_weight1_nceloss_weight0.1_unsync 2>&1 | tee log/backend.voxceleb2Naug_train_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76_spkloss_weight1_nceloss_weight0.1_unsync.spklossbest.log
(ING, finalbestspklossNacc) bash run_backendonly_cpuparallel.voxceleb1.sh --model snapshot.ep.47 --nj 70 --ngpu 0 --expdir /export/b15/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2Naug_train_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76_spkloss_weight1_nceloss_weight0.1_unsync 2>&1 | tee log/backend.voxceleb2Naug_train_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76_spkloss_weight1_nceloss_weight0.1_unsync.finalbestspklossNacc.ep.47.log
(DONE: 1.68 with lda_dim=350)
expdir=/export/b15/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2Naug_train_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76_spkloss_weight1_nceloss_weight0.1_unsync
model=currentspklossbest
for lda_dim in 150 200 250 300 350 400;do
    (bash run_backendonly_cpuparallel.varlda_dim.voxceleb1.sh --lda_dim ${lda_dim} --stage 6 --nj 70 --ngpu 0 --expdir ${expdir} 2>&1 | tee log/backend.lda_dim${lda_dim}.$(basename ${expdir}).${model}.log) &
done
###### nceloss=1
(DONE: 1.99, current spklossbest) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir /export/b15/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2Naug_train_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76_spkloss_weight1_nceloss_weight1_unsync 2>&1 | tee log/backend.voxceleb2Naug_train_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76_spkloss_weight1_nceloss_weight1_unsync.spklossbest.log
(DONE: 1.77 with lda_dim=400)
expdir=/export/b15/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2Naug_train_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76_spkloss_weight1_nceloss_weight1_unsync
model=currentspklossbest
for lda_dim in 150 200 250 300 350 400;do
    (bash run_backendonly_cpuparallel.varlda_dim.voxceleb1.sh --lda_dim ${lda_dim} --stage 6 --nj 70 --ngpu 0 --expdir ${expdir} 2>&1 | tee log/backend.lda_dim${lda_dim}.$(basename ${expdir}).${model}.log) &
done
#### spkid-tts
##### dunit 512 (original)
######
(spkbest) bash run_backendonly_cpuparallel.voxceleb1.sh --model snapshot.ep.16 --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0.03_fullyunsync 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0.03_fullyunsync.spkbest.ep.16.log
(reconbest) bash run_backendonly_cpuparallel.voxceleb1.sh --model snapshot.ep.24 --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0.03_fullyunsync 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0.03_fullyunsync.spkrecon.ep.24.log
###### voxceleb2Naug
##### from 1e-4_initopt snapshot.ep.12
(DONE: 2.04, bestmseloss, snapshot.ep.25) bash run_backendonly_cpuparallel.voxceleb1.sh --model snapshot.ep.25 --nj 70 --ngpu 0 --expdir /export/b15/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2Naug_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_cleanrecon_lr1e-5_initopt_spkloss_weight0.03_fullyunsync_cleanrecon/ 2>&1 | tee log/backend.voxceleb2Naug_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_cleanrecon_lr1e-5_initopt_spkloss_weight0.03_fullyunsync_cleanrecon.bestmseloss.ep.25.log
(DONE: 1.99 best with lda_dim=350)
expdir=/export/b15/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2Naug_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_cleanrecon_lr1e-5_initopt_spkloss_weight0.03_fullyunsync_cleanrecon/
model=snapshot.ep.25
model_selection=bestmseloss
for lda_dim in 150 200 250 300 350 400;do
    if [[ ${model} == *".ep."* ]]; then
        (bash run_backendonly_cpuparallel.varlda_dim.voxceleb1.sh --lda_dim ${lda_dim} --model ${model} --stage 6 --nj 70 --ngpu 0 --expdir ${expdir} 2>&1 | tee log/backend.lda_dim${lda_dim}.$(basename ${expdir}).${model_selection}.${model#snapshot.}.log) &
    else
        (bash run_backendonly_cpuparallel.varlda_dim.voxceleb1.sh --lda_dim ${lda_dim} --stage 6 --nj 70 --ngpu 0 --expdir ${expdir} 2>&1 | tee log/backend.lda_dim${lda_dim}.$(basename ${expdir}).${model_selection}.log) &
    fi
done
##### from 1e-4_initopt snapshot.ep.21
(DONE: 2.01, bestreconlosses, snapshot.ep.28) bash run_backendonly_cpuparallel.voxceleb1.sh --model snapshot.ep.28 --nj 70 --ngpu 0 --expdir /export/b15/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2Naug_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_cleanrecon_lr1e-5_initopt_fromep21_spkloss_weight0.03_fullyunsync_cleanrecon/ 2>&1 | tee log/backend.voxceleb2Naug_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_cleanrecon_lr1e-5_initopt_fromep21_spkloss_weight0.03_fullyunsync_cleanrecon.bestreconlosses.ep.28.log
(DONE: 1.91 best with lda_dim=300)
expdir=/export/b15/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2Naug_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_cleanrecon_lr1e-5_initopt_fromep21_spkloss_weight0.03_fullyunsync_cleanrecon/
model=snapshot.ep.28
model_selection=bestreconlosses
for lda_dim in 150 200 250 300 350 400;do
    if [[ ${model} == *".ep."* ]]; then
        (bash run_backendonly_cpuparallel.varlda_dim.voxceleb1.sh --lda_dim ${lda_dim} --model ${model} --stage 6 --nj 70 --ngpu 0 --expdir ${expdir} 2>&1 | tee log/backend.lda_dim${lda_dim}.$(basename ${expdir}).${model_selection}.${model#snapshot.}.log) &
    else
        (bash run_backendonly_cpuparallel.varlda_dim.voxceleb1.sh --lda_dim ${lda_dim} --stage 6 --nj 70 --ngpu 0 --expdir ${expdir} 2>&1 | tee log/backend.lda_dim${lda_dim}.$(basename ${expdir}).${model_selection}.log) &
    fi
done
##### dunit 1024
(DONE: 2.38, current bestloss, ep.9) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_fullyunsync_shufflebatching_spkloss_weight0.1_fullyunsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_fullyunsync_shufflebatching_spkloss_weight0.1_fullyunsync.currentbestloss.ep.9.log
##### different lda_dim WITH current bestloss, ep.9 above
(DONE: 2.20 best with lda_dim=400)
for lda_dim in 150 200 250 300 350 400;do
    (bash run_backendonly_cpuparallel.varlda_dim.voxceleb1.sh --lda_dim ${lda_dim} --stage 6 --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_fullyunsync_shufflebatching_spkloss_weight0.1_fullyunsync/ 2>&1 | tee log/backend.lda_dim${lda_dim}.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_fullyunsync_shufflebatching_spkloss_weight0.1_fullyunsync.currentbestloss.ep.9.log) &
done
#### spkid-tts + nce
##### dunit 512 (original)
(DONE: 1.98) bash run_backendonly_cpuparallel.voxceleb1.sh --model snapshot.ep.29 --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56_lr1e-4_spkloss_weight0.03_nceloss_weight0.001_fullyunsync 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56_lr1e-4_spkloss_weight0.03_nceloss_weight0.001_fullyunsync.reconlossesbest.ep.29.log
##### dunit 1024
###### current best model (Extracted embeddings are now modified after the experiment so I need to extract them again if needed)
(DONE: 2.15, current bestloss, ep.11) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_nceloss_fullyunsync_shufflebatching_bs52_spkloss_weight0.03_nceloss_weight0.001_fullyunsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_nceloss_fullyunsync_shufflebatching_bs52_spkloss_weight0.03_nceloss_weight0.001_fullyunsync.currentbestloss.ep.11.log
##### different lda_dim WITH current bestloss, ep.11 above
(DONE: 1.98 best with lda_dim=300)
for lda_dim in 150 200 250 300 350 400;do
    (bash run_backendonly_cpuparallel.varlda_dim.voxceleb1.sh --lda_dim ${lda_dim} --stage 6 --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_nceloss_fullyunsync_shufflebatching_bs52_spkloss_weight0.03_nceloss_weight0.001_fullyunsync/ 2>&1 | tee log/backend.lda_dim${lda_dim}.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_nceloss_fullyunsync_shufflebatching_bs52_spkloss_weight0.03_nceloss_weight0.001_fullyunsync.currentbestloss.ep.11.log) &
done
###### final best model
(DONE: 2.09, bestalllossesBUTmseloss ep.16) bash run_backendonly_cpuparallel.voxceleb1.sh --model snapshot.ep.16 --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_nceloss_fullyunsync_shufflebatching_bs52_spkloss_weight0.03_nceloss_weight0.001_fullyunsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_nceloss_fullyunsync_shufflebatching_bs52_spkloss_weight0.03_nceloss_weight0.001_fullyunsync.finalbestalllossesBUTmseloss.ep.16.log
####### var lda_dim
(DONE: 1.90 best with lda_dim=300)
expdir=/export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_nceloss_fullyunsync_shufflebatching_bs52_spkloss_weight0.03_nceloss_weight0.001_fullyunsync/
model=snapshot.ep.16
model_selection=finalbestalllossesBUTmseloss
for lda_dim in 150 200 250 300 350 400;do
    if [[ ${model} == *".ep."* ]]; then
        (bash run_backendonly_cpuparallel.varlda_dim.voxceleb1.sh --lda_dim ${lda_dim} --model ${model} --stage 6 --nj 70 --ngpu 0 --expdir ${expdir} 2>&1 | tee log/backend.lda_dim${lda_dim}.$(basename ${expdir}).${model_selection}.${model#snapshot.}.log) &
    else
        (bash run_backendonly_cpuparallel.varlda_dim.voxceleb1.sh --lda_dim ${lda_dim} --stage 6 --nj 70 --ngpu 0 --expdir ${expdir} 2>&1 | tee log/backend.lda_dim${lda_dim}.$(basename ${expdir}).${model_selection}.log) &
    fi
done
###### to lr=1e-4 (from lr=1e-3) correctly
####### current
(DONE, current lossbest, ep.19) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_nceloss_fullyunsync_shufflebatching_bs52_lr1e-4_spkloss_weight0.03_nceloss_weight0.001_fullyunsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_nceloss_fullyunsync_shufflebatching_bs52_lr1e-4_spkloss_weight0.03_nceloss_weight0.001_fullyunsync.currentlossbest.ep.4.log
######## var lda_dim
(ING)
expdir=/export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_nceloss_fullyunsync_shufflebatching_bs52_lr1e-4_spkloss_weight0.03_nceloss_weight0.001_fullyunsync/
model=""
model_selection=currentlossbest
for lda_dim in 150 200 250 300 350 400;do
    if [[ ${model} == *".ep."* ]]; then
        (bash run_backendonly_cpuparallel.varlda_dim.voxceleb1.sh --lda_dim ${lda_dim} --model ${model} --stage 6 --nj 70 --ngpu 0 --expdir ${expdir} 2>&1 | tee log/backend.lda_dim${lda_dim}.$(basename ${expdir}).${model_selection}.${model#snapshot.}.log) &
    else
        (bash run_backendonly_cpuparallel.varlda_dim.voxceleb1.sh --lda_dim ${lda_dim} --stage 6 --nj 70 --ngpu 0 --expdir ${expdir} 2>&1 | tee log/backend.lda_dim${lda_dim}.$(basename ${expdir}).${model_selection}.log) &
    fi
done
####### final
(DONE: 1.97, lossbest, ep.24) bash run_backendonly_cpuparallel.voxceleb1.sh --model snapshot.ep.24 --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_nceloss_fullyunsync_shufflebatching_bs52_lr1e-4_spkloss_weight0.03_nceloss_weight0.001_fullyunsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_nceloss_fullyunsync_shufflebatching_bs52_lr1e-4_spkloss_weight0.03_nceloss_weight0.001_fullyunsync.finallossbest.ep.24.log
######## var lda_dim
(DONE: 1.81, lda_dim=250)
expdir=/export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_nceloss_fullyunsync_shufflebatching_bs52_lr1e-4_spkloss_weight0.03_nceloss_weight0.001_fullyunsync/
model="snapshot.ep.24"
model_selection=finallossbest
for lda_dim in 150 200 250 300 350 400;do
    if [[ ${model} == *".ep."* ]]; then
        (bash run_backendonly_cpuparallel.varlda_dim.voxceleb1.sh --lda_dim ${lda_dim} --model ${model} --stage 6 --nj 70 --ngpu 0 --expdir ${expdir} 2>&1 | tee log/backend.lda_dim${lda_dim}.$(basename ${expdir}).${model_selection}.${model#snapshot.}.log) &
    else
        (bash run_backendonly_cpuparallel.varlda_dim.voxceleb1.sh --lda_dim ${lda_dim} --stage 6 --nj 70 --ngpu 0 --expdir ${expdir} 2>&1 | tee log/backend.lda_dim${lda_dim}.$(basename ${expdir}).${model_selection}.log) &
    fi
done
###### to lr=1e-5 (from lr=1e-3) correctly
####### current
(DONE, current lossbest, ep.18) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_nceloss_fullyunsync_shufflebatching_bs52_lr1e-5_spkloss_weight0.03_nceloss_weight0.001_fullyunsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_nceloss_fullyunsync_shufflebatching_bs52_lr1e-5_spkloss_weight0.03_nceloss_weight0.001_fullyunsync.currentlossbest.ep.3.log
######## var lda_dim
(ING)
expdir=/export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_nceloss_fullyunsync_shufflebatching_bs52_lr1e-5_spkloss_weight0.03_nceloss_weight0.001_fullyunsync/
model=""
model_selection=currentlossbest
for lda_dim in 150 200 250 300 350 400;do
    if [[ ${model} == *".ep."* ]]; then
        (bash run_backendonly_cpuparallel.varlda_dim.voxceleb1.sh --lda_dim ${lda_dim} --model ${model} --stage 6 --nj 70 --ngpu 0 --expdir ${expdir} 2>&1 | tee log/backend.lda_dim${lda_dim}.$(basename ${expdir}).${model_selection}.${model#snapshot.}.log) &
    else
        (bash run_backendonly_cpuparallel.varlda_dim.voxceleb1.sh --lda_dim ${lda_dim} --stage 6 --nj 70 --ngpu 0 --expdir ${expdir} 2>&1 | tee log/backend.lda_dim${lda_dim}.$(basename ${expdir}).${model_selection}.log) &
    fi
done
####### final
(ING, lossbest, ep.26) bash run_backendonly_cpuparallel.voxceleb1.sh --model snapshot.ep.26 --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_nceloss_fullyunsync_shufflebatching_bs52_lr1e-5_spkloss_weight0.03_nceloss_weight0.001_fullyunsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_nceloss_fullyunsync_shufflebatching_bs52_lr1e-5_spkloss_weight0.03_nceloss_weight0.001_fullyunsync.finallossbest.ep.26.log
###### to lr=1e-6 (from lr=1e-3) correctly
####### current
(DONE, current lossbest, ep.19) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_nceloss_fullyunsync_shufflebatching_bs52_lr1e-6_spkloss_weight0.03_nceloss_weight0.001_fullyunsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_nceloss_fullyunsync_shufflebatching_bs52_lr1e-6_spkloss_weight0.03_nceloss_weight0.001_fullyunsync.currentlossbest.ep.4.log
######## var lda_dim
(ING)
expdir=/export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_nceloss_fullyunsync_shufflebatching_bs52_lr1e-6_spkloss_weight0.03_nceloss_weight0.001_fullyunsync/
model=""
model_selection=currentlossbest
for lda_dim in 150 200 250 300 350 400;do
    if [[ ${model} == *".ep."* ]]; then
        (bash run_backendonly_cpuparallel.varlda_dim.voxceleb1.sh --lda_dim ${lda_dim} --model ${model} --stage 6 --nj 70 --ngpu 0 --expdir ${expdir} 2>&1 | tee log/backend.lda_dim${lda_dim}.$(basename ${expdir}).${model_selection}.${model#snapshot.}.log) &
    else
        (bash run_backendonly_cpuparallel.varlda_dim.voxceleb1.sh --lda_dim ${lda_dim} --stage 6 --nj 70 --ngpu 0 --expdir ${expdir} 2>&1 | tee log/backend.lda_dim${lda_dim}.$(basename ${expdir}).${model_selection}.log) &
    fi
done
####### final
(ING, lossbest, ep.24) bash run_backendonly_cpuparallel.voxceleb1.sh --model snapshot.ep.24 --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_nceloss_fullyunsync_shufflebatching_bs52_lr1e-6_spkloss_weight0.03_nceloss_weight0.001_fullyunsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_nceloss_fullyunsync_shufflebatching_bs52_lr1e-6_spkloss_weight0.03_nceloss_weight0.001_fullyunsync.finallossbest.ep.24.log
###### to lr=1e-7 (from lr=1e-3) correctly
####### final
(ING, lossbest, ep.26) bash run_backendonly_cpuparallel.voxceleb1.sh --model snapshot.ep.26 --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_nceloss_fullyunsync_shufflebatching_bs52_lr1e-7_spkloss_weight0.03_nceloss_weight0.001_fullyunsync/ 2>&1 | tee log/backend.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm1024Nusecat_nceloss_fullyunsync_shufflebatching_bs52_lr1e-7_spkloss_weight0.03_nceloss_weight0.001_fullyunsync.finallossbest.ep.26.log

#### Combine embeddings from SPKID only + SPKID-TTS (1.94 + 1.89)
##### Combine embeddings
mkdir -p /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/bestspkidonlynce+bestspkidttsnce
paste-feats ark:dump/voxceleb1_train_filtered_trainNdev/emb.voxceleb2_train_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76_spkloss_weight1_nceloss_weight0.1_unsync ark:dump/voxceleb1_train_filtered_trainNdev/emb.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56_lr1e-4_spkloss_weight0.03_nceloss_weight0.001_fullyunsync_snapshot.ep.29 ark,t:dump/voxceleb1_train_filtered_trainNdev/emb.bestspkidonlynce+bestspkidttsnce
paste-feats ark:dump/voxceleb1_test/emb.voxceleb2_train_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76_spkloss_weight1_nceloss_weight0.1_unsync ark:dump/voxceleb1_test/emb.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_nceloss_fullyunsync_shufflebatching_bs56_lr1e-4_spkloss_weight0.03_nceloss_weight0.001_fullyunsync_snapshot.ep.29 ark,t:dump/voxceleb1_test/emb.bestspkidonlynce+bestspkidttsnce
# *** Then I run ":%s/ \[\n /\[/g" on two newly generated embedding files to prevent error when using some kaldi binary (it seems to be recognized as a vector wtihout the change)
##### Actual run from stage 6
###### lda_dim=150
(DONE: 1.97) bash run_backendonly_cpuparallel.voxceleb1.sh --stage 6 --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/bestspkidonlynce+bestspkidttsnce 2>&1 | tee log/backend.bestspkidonlynce+bestspkidttsnce.estspkidonlynce+bestspkidttsnce.log
###### var lda_dim
(DONE)
for lda_dim in 150 200 250 300 350 400 450 500 550 600 650 700 750 800;do
    (bash run_backendonly_cpuparallel.varlda_dim.voxceleb1.sh --lda_dim ${lda_dim} --stage 6 --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/bestspkidonlynce+bestspkidttsnce 2>&1 | tee log/backend.lda_dim${lda_dim}.bestspkidonlynce+bestspkidttsnce.log ) &
done
#### Combine embeddings from SPKID only + TTS only (1.94 + )
mkdir -p /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/bestspkidonlynce+bestttsonly
paste-feats ark:dump/voxceleb1_train_filtered_trainNdev/emb.voxceleb2_train_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76_spkloss_weight1_nceloss_weight0.1_unsync ark:dump/voxceleb1_train_filtered_trainNdev/emb.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0_fullyunsync ark,t:dump/voxceleb1_train_filtered_trainNdev/emb.bestspkidonlynce+bestttsonly
paste-feats ark:dump/voxceleb1_test/emb.voxceleb2_train_train_pytorch_tacotron2+spkemb_spkidonly_nceloss_shufflebatching_bs76_spkloss_weight1_nceloss_weight0.1_unsync ark:dump/voxceleb1_test/emb.voxceleb2_train_train_pytorch_forwardtacotron2+spkemb_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0_fullyunsync ark,t:dump/voxceleb1_test/emb.bestspkidonlynce+bestttsonly
(ING) bash run_backendonly_cpuparallel.voxceleb1.sh --stage 6 --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/bestspkidonlynce+bestttsonly 2>&1 | tee log/backend.bestspkidonlynce+bestttsonly.bestspkidonlynce+bestttsonly.log
###### var lda_dim
(ING)
for lda_dim in 150 200 250 300 350 400 450 500 550 600 650 700 750 800;do
    (bash run_backendonly_cpuparallel.varlda_dim.voxceleb1.sh --lda_dim ${lda_dim} --stage 6 --nj 70 --ngpu 0 --expdir /export/b14/jcho/espnet3/egs/libritts/asrtts+spkid_voxceleb2_phonealign/exp/bestspkidonlynce+bestttsonly 2>&1 | tee log/backend.lda_dim${lda_dim}.bestspkidonlynce+bestttsonly.log ) &
done

## Final eval. on different trial lists (Models marked * in the front of the line were used for the experiments below)
### spkidonly
(ING) bash run_backendevalonly_cpuparallel.voxceleb1_onallvoxcelebcleanedtriallists.sh --nj 70 --expdir exp/voxceleb1_train_filtered_train_pytorch_train_pytorch_tacotron2+spkemb_spkidonly_shufflebatching_bs128_lr1e-5_ep110_speakeridonly_ttslossw0_spkloss_weight1_final_unsync200to400random/
### spkidonly with nceloss
(DONE) bash run_backendevalonly_cpuparallel.voxceleb1_onallvoxcelebcleanedtriallists.sh --expdir exp/voxceleb1_train_filtered_train_pytorch_spkidonly_shufflebatching_bs76_nceloss_weight0.03_unsync200to400random
### spkid-tts (2flstm512 + shufflebatching)
(ING) bash run_backendevalonly_cpuparallel.voxceleb1_onallvoxcelebcleanedtriallists.sh --nj 70 --expdir exp/voxceleb1_train_filtered_train_pytorch_train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0.03
### spkid-tts with nceloss (2flstm512 + shufflebatching with nceloss=0.001 and bs=56)
(DONE) bash run_backendevalonly_cpuparallel.voxceleb1_onallvoxcelebcleanedtriallists.sh --model snapshot.ep.28 --nj 70 --expdir exp/voxceleb1_train_filtered_train_pytorch_forwardtacotron2+spkemb_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_bs56_checkafterrefix_spkloss_weight0.03_nceloss_weight0.001

# 2-2 Cosine scoring
bash run_cosbackend_cpuparallel.voxceleb1.sh 








# debug (top lines are more recent)
bash run.asrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.debug.pytorch_scheduler.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_bs5_lr_scheduler.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --gpu_ix $(free-gpu)
bash run.asrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.debug.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_bs5.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 --gpu_ix $(free-gpu)
bash run.spkidonly_nceloss.debug.sh --gpu_ix $(free-gpu) --nceloss_weight 0.003 --ngpu 1 --ttsloss_weight 0 --spkloss_weight 1 --stage 4 --n_average 0 --stop_stage 4
bash run.spkidonly_nceloss.debug.sh --gpu_ix $(free-gpu) --train_config conf/train_pytorch_tacotron2+spkemb_spkidonly_nceloss_improve_reportnceval_shufflebatching_bs76.yaml --nceloss_weight 1 --ngpu 1 --ttsloss_weight 0 --spkloss_weight 0 --stage 4 --n_average 0 --stop_stage 4
## stop at an error: ******* I think it is great in many cases (e.g., when you got some error after a few iterations, you do not need to run the iterations yourself manually). new way of decoding, do not set break points but will stop at the error while ipdb is still on. ********* NOTE **********: You have to enter "c" in the beginning.
(ING) bash run.asrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.consloss.debug.sh --gpu_ix $(free-gpu) --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_consloss_cosdis.yaml --stage 4 --stop_stage 4 --spkloss_weight 0.03 --consloss_weight 10
