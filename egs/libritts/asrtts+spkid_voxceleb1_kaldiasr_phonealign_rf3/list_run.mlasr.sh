# < front-end >
## spkloss_w = 0 (*********** ONLY for the first run since this starts from stage 4 *************)
(STOPPED (wrong self.it so rerun below only for stage 4), fixed bs w/ shuf. fullyunsync 2flstm512 & use_concate=true) bash run.mlasrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching.yaml --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0 2>&1 | tee log/run.mlasrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.fromstart2stage4.forwardtacotron2_2flstm512Nusecat_fullyunsync_shufflebatching.log
(DONE) bash run.mlasrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0 2>&1 | tee log/run.mlasrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.onlystage4.forwardtacotron2_2flstm512Nusecat_fullyunsync_shufflebatching.log
(ING resuming w/ lr1e-4 to 60 ep (self.it=61000 (self.lamb=5 already)), 2flstm512 & use_concate=true) bash run.mlasrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.sh --resume exp/voxceleb1_train_filtered_train_mlasr_pytorch_train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0/results/snapshot.ep.29 --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_lr1e-4_ep60.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0 2>&1 | tee log/run.asrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.onlystage4.forwardtacotron2_2flstm512Nusecat_shufflebatching_fullyunsync.fromep29to60wlr1e-4log
## spkloss_w = 0.3
(ING: once the above run is done til stage 3, fixed bs w/ shuf. fullyunsync 2flstm512 & use_concate=true) bash run.mlasrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.3 2>&1 | tee log/run.mlasrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.3.onlystage4.forwardtacotron2_2flstm512Nusecat_fullyunsync_shufflebatching.log
## spkloss_w = 0.1
(DONE: once the above run is done til stage 3, fixed bs w/ shuf. fullyunsync 2flstm512 & use_concate=true) bash run.mlasrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.1 2>&1 | tee log/run.mlasrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.1.onlystage4.forwardtacotron2_2flstm512Nusecat_fullyunsync_shufflebatching.log
## spkloss_w = 0.03
(ING: once the above run is done til stage 3, fixed bs w/ shuf. fullyunsync 2flstm512 & use_concate=true) bash run.mlasrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.03 2>&1 | tee log/run.mlasrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.03.onlystage4.forwardtacotron2_2flstm512Nusecat_fullyunsync_shufflebatching.log
## spkloss_w = 0.01
(ING: once the above run is done til stage 3, fixed bs w/ shuf. fullyunsync 2flstm512 & use_concate=true) bash run.mlasrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.01 2>&1 | tee log/run.mlasrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.01.onlystage4.forwardtacotron2_2flstm512Nusecat_fullyunsync_shufflebatching.log
## spkloss_w = 0.003
(ING: once the above run is done til stage 3, fixed bs w/ shuf. fullyunsync 2flstm512 & use_concate=true) bash run.mlasrttsspkid.spkloss_weight.new.update.rf1.forwardtacotron2.sh --train_config conf/train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching.yaml --stage 4 --stop_stage 4 --ngpu 1 --n_average 0 --spkloss_weight 0.003 2>&1 | tee log/run.mlasrttsspkid.spkloss_weight.new.update.rf1.spkloss_weight0.003.onlystage4.forwardtacotron2_2flstm512Nusecat_fullyunsync_shufflebatching.log

# < back-end >
(DONE, finallossbest (ep.29)) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_train_mlasr_pytorch_train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0.03 2>&1 | tee log/backend.voxceleb1_train_filtered_train_mlasr_pytorch_train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0.03.finallossbest.ep.29.log
(ING, finallossbest (ep.28)) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_train_mlasr_pytorch_train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0.01 2>&1 | tee log/backend.voxceleb1_train_filtered_train_mlasr_pytorch_train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0.01.finallossbest.ep.28.log
(DONE, finallossbest (ep.29)) bash run_backendonly_cpuparallel.voxceleb1.sh --nj 70 --ngpu 0 --expdir exp/voxceleb1_train_filtered_train_mlasr_pytorch_train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0 2>&1 | tee log/backend.voxceleb1_train_filtered_train_mlasr_pytorch_train_pytorch_forwardtacotron2+spkemb_noatt_rf1_2flstm512Nusecat_fullyunsync_shufflebatching_spkloss_weight0.finallossbest.ep.29.log
